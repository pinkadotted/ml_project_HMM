{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning\n",
    "## Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Report the precision, recall and F scores of such a baseline system for each dataset:\n",
    "- EN dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.5348\n",
    "    - Entity  recall: 0.7656\n",
    "    - Entity  F: 0.6297\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.3902\n",
    "    - Sentiment  recall: 0.5586\n",
    "    - Sentiment  F: 0.4595\n",
    "- FR dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.1670\n",
    "    - Entity  recall: 0.7815\n",
    "    - Entity  F: 0.2751\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.0709\n",
    "    - Sentiment  recall: 0.3319\n",
    "    - Sentiment  F: 0.1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in the filename for the training data\n",
    "# returns word_sequences, tag_sequences\n",
    "# word_sequences is a list in the form: [ [x_1_1, x_1_2, ...], [x_2_1, x_2_2, ...], ... [x_m_1, x_m_2, ... ] ]\n",
    "# tag_sequences is a list in the form: [ [y_1_1, y_1_2, ...], [y_2_1, y_2_2, ...], ... [y_m_1, y_m_2, ... ] ]\n",
    "def read_training_data(training_filename):\n",
    "    training_file = open(training_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    word_sequences = []\n",
    "    tag_sequences = []\n",
    "    \n",
    "    current_word_sequence = []\n",
    "    current_tag_sequence = []\n",
    "    \n",
    "    for line in training_file:\n",
    "        training_word_and_tag = line.strip().split(\" \")\n",
    "        \n",
    "        # add the current word and tag to the current word sequence and current tag sequence\n",
    "        if (len(training_word_and_tag) == 2):\n",
    "            current_word_sequence += [training_word_and_tag[0]]\n",
    "            current_tag_sequence += [training_word_and_tag[1]]\n",
    "        \n",
    "        # if the sentence ended (empty line), add the previous word sequence and tag sequence to the lists of\n",
    "        # word sequences and tag sequences respectively.\n",
    "        else:\n",
    "            word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "            tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "            \n",
    "            current_word_sequence = []\n",
    "            current_tag_sequence = []\n",
    "            \n",
    "    # account for the last word sequence\n",
    "    if (len(current_word_sequence) != 0):\n",
    "        word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "        tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "        \n",
    "    training_file.close()\n",
    "\n",
    "    return word_sequences, tag_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename for the test data\n",
    "# returns the test data as a list in the form: [ [x1_1, x1_2, ...], [x2_1, x2_2, ...] ]\n",
    "def read_test_data(test_filename):\n",
    "    test_file = open(test_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    test_word_sequences = []\n",
    "    \n",
    "    current_test_word_sequence = []\n",
    "\n",
    "    for line in test_file:\n",
    "        test_word = line.strip()\n",
    "        \n",
    "        # add current word to the current word sequence\n",
    "        if (len(test_word) != 0):\n",
    "            current_test_word_sequence += [test_word]\n",
    "            \n",
    "        # if sentence ended (len(test_word) == 0)\n",
    "        else:\n",
    "            test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "            current_test_word_sequence = []\n",
    "        \n",
    "    # account for the last word sequence\n",
    "    if (len(current_test_word_sequence) != 0):\n",
    "        test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "\n",
    "    test_file.close()\n",
    "\n",
    "    return test_word_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename of the training data and optional k value\n",
    "# returns emission_dict, tags_list\n",
    "# emission_dict[x][y] gives the value e(x|y)\n",
    "def create_emission_dict_tags_list(training_filename, k=1):\n",
    "    # emission_dict[x][y] gives the value e(x|y)\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # count_y_dict[y] gives the total number of words tagged as y\n",
    "    count_y_dict = {} \n",
    "\n",
    "    # count_x_tagged_as_y_dict[x_i][y_j] gives the number of times each observed variable x_i\n",
    "    # was tagged as state y_j in the training data\n",
    "    count_x_tagged_as_y_dict = {}\n",
    "\n",
    "    # read training data\n",
    "    word_sequences, tags_sequences = read_training_data(training_filename)\n",
    "\n",
    "    # fill up count_y_dict and count_x_tagged_as_y_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(y in count_y_dict.keys()):\n",
    "                count_y_dict[y] = 0\n",
    "\n",
    "            count_y_dict[y] += 1\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in count_x_tagged_as_y_dict.keys()): \n",
    "                count_x_tagged_as_y_dict[x] = {}\n",
    "                \n",
    "            if not(y in count_x_tagged_as_y_dict[x].keys()):\n",
    "                count_x_tagged_as_y_dict[x][y] = 0\n",
    "\n",
    "            count_x_tagged_as_y_dict[x][y] += 1\n",
    "        \n",
    "    tags_list = count_y_dict.keys()\n",
    "\n",
    "    # fill up emission_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "        \n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in emission_dict.keys()):\n",
    "                emission_dict[x] = {}\n",
    "\n",
    "            emission_dict[x][y] = count_x_tagged_as_y_dict[x][y] / (count_y_dict[y] + k)\n",
    "            \n",
    "            # add entry for \"START\" and \"END\"\n",
    "            emission_dict[x][\"START\"] = 0\n",
    "            emission_dict[x][\"END\"] = 0\n",
    "\n",
    "    # add entry for #UNK#\n",
    "    emission_dict[\"#UNK#\"] = {}\n",
    "    \n",
    "    for tag in tags_list: # iterate over all the tags used in training\n",
    "        emission_dict[\"#UNK#\"][tag] = k / (count_y_dict[tag] + k)\n",
    "\n",
    "    return emission_dict, tags_list\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns emission parameter e(x|y)\n",
    "def emission(emission_dict, tags_list, x, y):\n",
    "    # if tag was not in training data\n",
    "    if (not(y in tags_list)):\n",
    "        print(\"This tag was not in the training data\")\n",
    "        result = 0\n",
    "\n",
    "    # else if word was not in training data\n",
    "    elif (not(x in emission_dict.keys())): # treat x as \"#UNK#\"\n",
    "        result = emission_dict[\"#UNK#\"][y] # result = k / (count_y_dict[y] + k)\n",
    "        \n",
    "    # else if word is was in training data\n",
    "    else:\n",
    "        # if x was never tagged as y before during training, the probability is 0\n",
    "        if not(y in emission_dict[x].keys()):\n",
    "            emission_dict[x][y] = 0\n",
    "\n",
    "        result = emission_dict[x][y] \n",
    "\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in a filename and a list of results in the form: [ [x1, tag1], [x2, tag2], ...]\n",
    "# writes the results to a file specified by the filename\n",
    "def write_result(result_filename, word_sequences, tag_sequences):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            result_file.write(word_sequences[sequence_index][word_index] + \" \" + tag_sequences[sequence_index][word_index] + \"\\n\")\n",
    "        \n",
    "        result_file.write(\"\\n\")\n",
    "\n",
    "    result_file.close()\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# function that takes in the filenames for the training data and test data\n",
    "# produces the tag y* = arg_max_y e(x|y) for each word in the test data\n",
    "# writes the results to a file specified by the filename\n",
    "# returns the results as a list in the form: [ [x1, y*1], [x2, y*2], ... ]\n",
    "def simple_sentiment_analysis(training_filename, test_filename, result_filename, k=1):    \n",
    "    # initialise emission_dict\n",
    "    emission_dict, tags_list = create_emission_dict_tags_list(training_filename, k)\n",
    "    \n",
    "    test_word_sequences = read_test_data(test_filename)\n",
    "    \n",
    "    prediction_tag_sequences = []\n",
    "\n",
    "    for test_word_sequence in test_word_sequences:\n",
    "        current_prediction_tag_sequence = []\n",
    "        \n",
    "        for test_word in test_word_sequence:\n",
    "            # find the tag that gives the highest value for e(test_variable | tag)\n",
    "            predicted_tag = \"\"\n",
    "            highest_emission_value = 0\n",
    "\n",
    "            for tag in tags_list:\n",
    "                current_emission_value = emission(emission_dict, tags_list, test_word, tag)\n",
    "\n",
    "                if current_emission_value > highest_emission_value:\n",
    "                    highest_emission_value = current_emission_value\n",
    "                    predicted_tag = tag\n",
    "\n",
    "            current_prediction_tag_sequence += [predicted_tag]\n",
    "            \n",
    "        # at the end of the sentence, add the current prediction tag sequence to the lise prediction_tag_sequences\n",
    "        prediction_tag_sequences += [copy.deepcopy(current_prediction_tag_sequence)]\n",
    "        current_prediction_tag_sequence = []\n",
    "\n",
    "    write_result(result_filename, test_word_sequences, prediction_tag_sequences)\n",
    "\n",
    "    return test_word_sequences, prediction_tag_sequences, emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 802\n",
      "#Entity in prediction: 1148\n",
      "\n",
      "#Correct Entity : 614\n",
      "Entity  precision: 0.5348\n",
      "Entity  recall: 0.7656\n",
      "Entity  F: 0.6297\n",
      "\n",
      "#Correct Sentiment : 448\n",
      "Sentiment  precision: 0.3902\n",
      "Sentiment  recall: 0.5586\n",
      "Sentiment  F: 0.4595\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the EN dataset\n",
    "en_results = simple_sentiment_analysis(\"EN/train\", \"EN/dev.in\", \"EN/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 238\n",
      "#Entity in prediction: 1114\n",
      "\n",
      "#Correct Entity : 186\n",
      "Entity  precision: 0.1670\n",
      "Entity  recall: 0.7815\n",
      "Entity  F: 0.2751\n",
      "\n",
      "#Correct Sentiment : 79\n",
      "Sentiment  precision: 0.0709\n",
      "Sentiment  recall: 0.3319\n",
      "Sentiment  F: 0.1169\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the FR dataset\n",
    "fr_results = simple_sentiment_analysis(\"FR/train\", \"FR/dev.in\", \"FR/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case emission_dict:\n",
      "{'word1': {'tag1': 0.75, 'START': 0, 'END': 0, 'tag2': 0.2, 'tag3': 0}, 'word2': {'tag2': 0.6, 'START': 0, 'END': 0, 'tag1': 0, 'tag3': 0}, 'word3': {'tag3': 0.5, 'START': 0, 'END': 0, 'tag1': 0, 'tag2': 0}, '#UNK#': {'tag1': 0.25, 'tag2': 0.2, 'tag3': 0.5}}\n",
      "\n",
      "Test case passed: True\n"
     ]
    }
   ],
   "source": [
    "# part 1 test case\n",
    "# create training data for test case\n",
    "test_case_train_file = open(\"p1_test_train\", \"w\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word3 tag3\")\n",
    "test_case_train_file.close()\n",
    "\n",
    "# create test data for test case\n",
    "test_case_test_file = open(\"p1_test_in\", \"w\")\n",
    "test_case_test_file.write(\"word1\\n\")\n",
    "test_case_test_file.write(\"word2\\n\")\n",
    "test_case_test_file.write(\"word3\\n\")\n",
    "test_case_test_file.write(\"unknown_word\")\n",
    "test_case_test_file.close()\n",
    "\n",
    "# create expected output for test case\n",
    "test_case_expected_file = open(\"p1_test_out\", \"w\")\n",
    "test_case_expected_file.write(\"word1 tag1\\n\")\n",
    "test_case_expected_file.write(\"word2 tag2\\n\")\n",
    "test_case_expected_file.write(\"word3 tag3\\n\")\n",
    "test_case_expected_file.write(\"unknown_word tag3\")\n",
    "test_case_expected_file.close()\n",
    "\n",
    "# perform the test\n",
    "test_word_sequences, prediction_tag_sequences, emission_dict = simple_sentiment_analysis(\"p1_test_train\", \"p1_test_in\", \"p1_test_prediction\")\n",
    "test_word_sequences, expected_tag_sequences= read_training_data(\"p1_test_out\")\n",
    "\n",
    "# show results for the test\n",
    "print(\"\\nTest case emission_dict:\")\n",
    "print(emission_dict)\n",
    "print(\"\")\n",
    "\n",
    "test_case_passed = True\n",
    "\n",
    "for sequence_index in range(0, len(test_word_sequences)):\n",
    "    for tag_index in range(0, len(test_word_sequences[sequence_index])):\n",
    "        if prediction_tag_sequences[sequence_index][tag_index] != expected_tag_sequences[sequence_index][tag_index]:\n",
    "            test_case_passed = False\n",
    "            \n",
    "            print(\"Test case failed.\")\n",
    "            print(f\"Word: {test_case_prediction[i][0]}\")\n",
    "            print(f\"Tag: {test_case_prediction[i][1]}\")\n",
    "            print(f\"Expected tag: {test_case_expected[i][1]}\\n\")\n",
    "\n",
    "print(f\"Test case passed: {test_case_passed}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Report the precision, recall and F scores of such a baseline system for each dataset:\n",
    "- EN dataset\n",
    "  - Entity scores:\n",
    "    - #Entity in gold data: 802\n",
    "    - #Entity in prediction: 793\n",
    "    - #Correct Entity : 554\n",
    "    - Entity  precision: 0.6986\n",
    "    - Entity  recall: 0.6908\n",
    "    - Entity  F: 0.6947\n",
    "  - Sentiment scores:\n",
    "    - #Correct Sentiment: 500\n",
    "    - Sentiment  precision: 0.6305\n",
    "    - Sentiment  recall: 0.6234\n",
    "    - Sentiment  F: 0.6270\n",
    "- FR dataset\n",
    "  - Entity scores:\n",
    "    - #Correct Entity : 37\n",
    "    - Entity  precision: 0.6066\n",
    "    - Entity  recall: 0.1555\n",
    "    - Entity  F: 0.2475\n",
    "  - Sentiment scores:\n",
    "    - #Correct Sentiment : 20\n",
    "    - Sentiment  precision: 0.3279\n",
    "    - Sentiment  recall: 0.0840\n",
    "    - Sentiment  F: 0.1338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes in list of tag sequences in the form [ [y1_1, y1_2, ... ], [y2_1, y2_2, ...], ... ]\n",
    "# outputs transition_dict\n",
    "def create_transition_dict(input_list):\n",
    "    # Create transition dict\n",
    "    # ========================\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # Get list of tags\n",
    "    tags = set([tag for sentence in input_list for tag in sentence])\n",
    "\n",
    "    # Update probability for each instance of tag1 > tag2 inside transition_dict\n",
    "    for tag1 in tags:\n",
    "        for tag2 in tags:\n",
    "            count = 0\n",
    "            total = 0\n",
    "            for sentence in input_list:\n",
    "                total += len(sentence) - 1\n",
    "                for i in range(len(sentence) - 1):\n",
    "                    if sentence[i] == tag1 and sentence[i+1] == tag2:\n",
    "                        count += 1\n",
    "            if count > 0:\n",
    "                transition_dict[(tag1, tag2)] = count / total\n",
    "    \n",
    "    # Create initial probability dict\n",
    "    # ========================\n",
    "    start_tag_count_dict = {}\n",
    "    stop_tag_count_dict = {}\n",
    "    \n",
    "    # Get num of starting tags that appear\n",
    "    for sentence in input_list:\n",
    "        if sentence[0] not in start_tag_count_dict:\n",
    "            start_tag_count_dict[sentence[0]] = 1\n",
    "        else:\n",
    "            start_tag_count_dict[sentence[0]] += 1\n",
    "        \n",
    "        if sentence[-1] not in stop_tag_count_dict:\n",
    "            stop_tag_count_dict[sentence[-1]] = 1\n",
    "        else:\n",
    "            stop_tag_count_dict[sentence[-1]] += 1\n",
    "    \n",
    "    # Fill in initial prob dict with num of starting/ending tags divided by total sentence num\n",
    "    for tag in start_tag_count_dict:\n",
    "        transition_dict[('START',tag)] = start_tag_count_dict[tag] / len(input_list)\n",
    "    for tag in stop_tag_count_dict:\n",
    "        transition_dict[(tag,'STOP')] = stop_tag_count_dict[tag] / len(input_list)\n",
    "    \n",
    "    return transition_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns transition parameter q(yi|yi-1)\n",
    "def transition(transition_dict, yi_minus_1, yi):\n",
    "    if (yi_minus_1, yi) not in transition_dict.keys():\n",
    "        transition_dict[(yi_minus_1, yi)] = 0\n",
    "\n",
    "    result = transition_dict[(yi_minus_1, yi)]\n",
    "\n",
    "    return result\n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# creates the table of pi values\n",
    "def viterby_first_order(word_sequence, tags_list, emission_dict, transition_dict):\n",
    "    pi_dict = {}\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # initialization\n",
    "    pi_dict[0] = {}\n",
    "        \n",
    "    pi_dict[0][\"START\"] = math.log(1)\n",
    "    \n",
    "    for word_index in range(1, len(word_sequence)+1): # index for each word in each sentence\n",
    "        for tag in tags_list: # each tag\n",
    "            if word_index not in pi_dict.keys():\n",
    "                pi_dict[word_index] = {}\n",
    "\n",
    "            pi_dict[word_index][tag] = float('-inf') # initialize pi(j, u) = 0 for all j and u\n",
    "\n",
    "    # for the first word, transition from \"START\" to v\n",
    "    x_1 = word_sequence[0]\n",
    "    \n",
    "    for v in tags_list:\n",
    "        pi = pi_dict[0][\"START\"]\n",
    "        trans = transition(transition_dict, \"START\", v)\n",
    "        emi = emission(emission_dict, tags_list, x_1, v)\n",
    "\n",
    "        if trans != 0:\n",
    "            log_trans = math.log(trans)\n",
    "        else:\n",
    "            log_trans = float('-inf')\n",
    "\n",
    "        if emi != 0:\n",
    "            log_emi = math.log(emi)\n",
    "        else:\n",
    "            log_emi = float('-inf')\n",
    "\n",
    "        current_pi_val = pi + log_trans + log_emi\n",
    "\n",
    "        pi_dict[1][v] = current_pi_val\n",
    "        \n",
    "    # =============================================================================================================================\n",
    "    # intermediate steps\n",
    "    # for each observed variable\n",
    "    for j in range(1, len(word_sequence)):\n",
    "        x_j_plus_1 = word_sequence[j] # refers to the jth word (to calculate emission)\n",
    "        \n",
    "        # for each hidden state v\n",
    "        for v in tags_list:\n",
    "            \n",
    "            # pi(j+1, v) = max over all u { pi(j,u) * transition(u, v) * emissision(x_j_plus_1, v) }\n",
    "            max_pi_val = float('-inf')\n",
    "            \n",
    "            for u in tags_list:\n",
    "                pi = pi_dict[j][u]\n",
    "                trans = transition(transition_dict, u, v)\n",
    "                emi = emission(emission_dict, tags_list, x_j_plus_1, v)\n",
    "\n",
    "                if trans != 0:\n",
    "                    log_trans = math.log(trans)\n",
    "                else:\n",
    "                    log_trans = float('-inf')\n",
    "                \n",
    "                if emi != 0:\n",
    "                    log_emi = math.log(emi)\n",
    "                else:\n",
    "                    log_emi = float('-inf')\n",
    "                \n",
    "                current_pi_val = pi + log_trans + log_emi\n",
    "\n",
    "                # save the value that maximises\n",
    "                if (current_pi_val > max_pi_val):\n",
    "                    max_pi_val = current_pi_val\n",
    "        \n",
    "            pi_dict[j+1][v] = max_pi_val\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # final step\n",
    "    max_pi_val = float('-inf')\n",
    "    \n",
    "    # for each hidden state u\n",
    "    for u in tags_list:\n",
    "        pi = pi_dict[len(word_sequence)][u]\n",
    "        trans = transition(transition_dict, u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            log_trans = math.log(trans)\n",
    "        else:\n",
    "            log_trans = float('-inf')\n",
    "\n",
    "        current_pi_val = pi + log_trans\n",
    "        \n",
    "        # save the value that maximises\n",
    "        if (current_pi_val > max_pi_val):\n",
    "            max_pi_val = current_pi_val\n",
    "        \n",
    "    pi_dict[len(word_sequence) + 1] = {}\n",
    "    pi_dict[len(word_sequence) + 1][\"STOP\"]  = max_pi_val\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # BACKTRACKING\n",
    "    decoding_list = []\n",
    "    # check final layer argmax\n",
    "    argmax = float('-inf')\n",
    "    currentmax = float('-inf')\n",
    "    argmax_tag = \"NO_TAG_FOUND\"\n",
    "    \n",
    "    # start from last hidden variable\n",
    "    for u in tags_list:\n",
    "        pi = pi_dict[len(pi_dict)-2][u]\n",
    "        trans = transition(transition_dict, u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            log_trans = math.log(trans)\n",
    "        else:\n",
    "            log_trans = float(\"-inf\")\n",
    "        \n",
    "        currentmax = pi + log_trans\n",
    "        \n",
    "        if currentmax > argmax:\n",
    "            argmax = currentmax\n",
    "            argmax_tag = u\n",
    "        \n",
    "    decoding_list.append(argmax_tag)\n",
    "    \n",
    "    \n",
    "    # Backtrack rest of pi_dict\n",
    "    for j in range(len(pi_dict)-3, 0, -1):\n",
    "        \n",
    "        argmax = float('-inf')\n",
    "        currentmax = 0\n",
    "        argmax_index = 0\n",
    "    \n",
    "        for u in tags_list:\n",
    "    \n",
    "            pi = pi_dict[j][u]\n",
    "            trans = transition(transition_dict, u, decoding_list[-1])\n",
    "            \n",
    "            if trans != 0:\n",
    "                log_trans = math.log(trans)\n",
    "            else:\n",
    "                log_trans = float(\"-inf\")\n",
    "\n",
    "            currentmax = pi + log_trans\n",
    "\n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_tag = u\n",
    "        \n",
    "        decoding_list.append(argmax_tag)\n",
    "        \n",
    "    decoding_list = decoding_list[::-1]\n",
    "    \n",
    "    return decoding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_prediction(training_filename, test_filename, result_filename, k=1):\n",
    "    training_word_sequences, training_tag_sequences = read_training_data(training_filename)\n",
    "    test_word_sequences = read_test_data(test_filename)\n",
    "    \n",
    "    emission_dict, tags_list = create_emission_dict_tags_list(training_filename, k)\n",
    "    transition_dict = create_transition_dict(training_tag_sequences)\n",
    "\n",
    "    tag_sequences = []\n",
    "\n",
    "    for i in range(len(test_word_sequences)):\n",
    "        most_likely_tags = viterby_first_order(test_word_sequences[i], tags_list, emission_dict, transition_dict)\n",
    "        tag_sequences.append(most_likely_tags)\n",
    "    \n",
    "    # some words may still be tagged \"NO_TAG_FOUND\". For those words, select the tag that \n",
    "    # gives the highest emission value\n",
    "    for sentence_index in range(0, len(test_word_sequences)):\n",
    "        for word_index in range(0, len(test_word_sequences[sentence_index])):\n",
    "            # check the tag that was predicted for each word\n",
    "            checking_word = test_word_sequences[sentence_index][word_index]\n",
    "            checking_tag = tag_sequences[sentence_index][word_index]\n",
    "\n",
    "            if (checking_tag == \"NO_TAG_FOUND\"):\n",
    "                new_predicted_tag = \"\"\n",
    "                \n",
    "                # if the tag is \"NO_TAG_FOUND\", select the tag that gives the highest emission values\n",
    "                highest_emission_value = 0\n",
    "                \n",
    "                for tag in tags_list:\n",
    "                    current_emission_value = emission(emission_dict, tags_list, checking_word, tag)\n",
    "\n",
    "                    if current_emission_value > highest_emission_value:\n",
    "                        highest_emission_value = current_emission_value\n",
    "                        new_predicted_tag = tag\n",
    "                \n",
    "                tag_sequences[sentence_index][word_index] = new_predicted_tag\n",
    "\n",
    "        \n",
    "    write_result(result_filename, test_word_sequences, tag_sequences)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 802\n",
      "#Entity in prediction: 793\n",
      "\n",
      "#Correct Entity : 554\n",
      "Entity  precision: 0.6986\n",
      "Entity  recall: 0.6908\n",
      "Entity  F: 0.6947\n",
      "\n",
      "#Correct Sentiment : 500\n",
      "Sentiment  precision: 0.6305\n",
      "Sentiment  recall: 0.6234\n",
      "Sentiment  F: 0.6270\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the EN dataset\n",
    "viterbi_prediction(\"EN/train\", \"EN/dev.in\", \"EN/dev.p2.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 238\n",
      "#Entity in prediction: 61\n",
      "\n",
      "#Correct Entity : 37\n",
      "Entity  precision: 0.6066\n",
      "Entity  recall: 0.1555\n",
      "Entity  F: 0.2475\n",
      "\n",
      "#Correct Sentiment : 20\n",
      "Sentiment  precision: 0.3279\n",
      "Sentiment  recall: 0.0840\n",
      "Sentiment  F: 0.1338\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the FR dataset\n",
    "viterbi_prediction(\"FR/train\", \"FR/dev.in\", \"FR/dev.p2.out\")\n",
    "\n",
    "# evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p2.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tuple_dict_to_dict_of_dicts(tuple_dict):\n",
    "    dict_of_dicts = {}\n",
    "    \n",
    "    for key in tuple_dict.keys():\n",
    "        if key[0] not in dict_of_dicts:\n",
    "            dict_of_dicts[key[0]] = {}\n",
    "            \n",
    "        dict_of_dicts[key[0]][key[1]] = tuple_dict[key]\n",
    "        \n",
    "    return dict_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_dict_second_order(input_list):\n",
    "    \n",
    "    # Create transition dict\n",
    "    # ========================\n",
    "    transition_dict = {}\n",
    "    \n",
    "    for sentence in input_list:\n",
    "        \n",
    "        # ONLY RUN IF SENTENCE HAS MORE THAN 1 WORD\n",
    "        if len(sentence) > 1:\n",
    "            \n",
    "            for i in range(len(sentence) - 2):\n",
    "\n",
    "                current_state = sentence[i]\n",
    "                next_state = sentence[i + 1]\n",
    "                next_next_state = sentence[i + 2]\n",
    "\n",
    "                if current_state not in transition_dict:\n",
    "                    transition_dict[current_state] = {}\n",
    "\n",
    "                if next_state not in transition_dict[current_state]:\n",
    "                    transition_dict[current_state][next_state] = {}\n",
    "\n",
    "                if next_next_state not in transition_dict[current_state][next_state]:       \n",
    "                    transition_dict[current_state][next_state][next_next_state] = 0\n",
    "\n",
    "                transition_dict[current_state][next_state][next_next_state] += 1\n",
    "\n",
    "                \n",
    "    for current_state in transition_dict:\n",
    "        \n",
    "        for next_state in transition_dict[current_state]:\n",
    "            \n",
    "            total_count = sum(transition_dict[current_state][next_state].values())\n",
    "            \n",
    "            for next_next_state in transition_dict[current_state][next_state]:\n",
    "                transition_dict[current_state][next_state][next_next_state] /= total_count\n",
    "           \n",
    "        \n",
    "    # Create initial probability dict\n",
    "    # ========================\n",
    "    start_tag_count_dict = {}\n",
    "    stop_tag_count_dict = {}\n",
    "    \n",
    "    sentence_w_one_word = {}\n",
    "    \n",
    "    # Get num of starting tags that appear\n",
    "    for sentence in input_list:\n",
    "        \n",
    "        # ONLY RUN IF SENTENCE HAS MORE THAN 1 WORD\n",
    "        if len(sentence) > 1:\n",
    "            \n",
    "            if sentence[0] not in start_tag_count_dict:\n",
    "                start_tag_count_dict[sentence[0]] = {}\n",
    "\n",
    "            if sentence[1] not in start_tag_count_dict[sentence[0]]:\n",
    "                start_tag_count_dict[sentence[0]][sentence[1]] = 1\n",
    "            else:\n",
    "                start_tag_count_dict[sentence[0]][sentence[1]] += 1\n",
    "\n",
    "\n",
    "            if sentence[-2] not in stop_tag_count_dict:\n",
    "                stop_tag_count_dict[sentence[-2]] = {}\n",
    "\n",
    "            if sentence[-1] not in stop_tag_count_dict[sentence[-2]]:\n",
    "                stop_tag_count_dict[sentence[-2]][sentence[-1]] = 1\n",
    "            else:\n",
    "                stop_tag_count_dict[sentence[-2]][sentence[-1]] += 1\n",
    "        \n",
    "        else:\n",
    "            # FOR SENTENCES WITH ONLY ONE WORD\n",
    "            word = sentence[0]\n",
    "            \n",
    "            if word not in sentence_w_one_word:\n",
    "                sentence_w_one_word[word] = 0\n",
    "            sentence_w_one_word[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Fill in initial prob dict with num of starting/ending tags divided by total sentence num\n",
    "    for tag in start_tag_count_dict:\n",
    "        for tag2 in start_tag_count_dict[tag]:\n",
    "            \n",
    "            if 'START' not in transition_dict:\n",
    "                transition_dict['START'] = {}\n",
    "            if tag not in transition_dict['START']:\n",
    "                transition_dict['START'][tag] = {}\n",
    "                \n",
    "            transition_dict['START'][tag][tag2] = start_tag_count_dict[tag][tag2] / len(input_list)\n",
    "        \n",
    "        for tag in stop_tag_count_dict:\n",
    "             for tag2 in stop_tag_count_dict[tag]:\n",
    "                    \n",
    "                if tag not in transition_dict:\n",
    "                    transition_dict[tag] = {}\n",
    "                if tag2 not in transition_dict[tag]:\n",
    "                    transition_dict[tag][tag2] = {}\n",
    "            \n",
    "                transition_dict[tag][tag2]['STOP'] = stop_tag_count_dict[tag][tag2] / len(input_list)\n",
    "                \n",
    "    # Fill in initial prob dict with sentences with only ONE WORD\n",
    "    for tag in sentence_w_one_word:\n",
    "        if 'START' not in transition_dict:\n",
    "            transition_dict['START'] = {}\n",
    "        if 'START' not in transition_dict['START']:\n",
    "            transition_dict['START']['START'] = {}\n",
    "        if tag not in transition_dict['START']['START']:\n",
    "            transition_dict['START']['START'][tag] = sentence_w_one_word[tag] / len(input_list)\n",
    "        \n",
    "        if tag not in transition_dict['START']:\n",
    "            transition_dict['START'][tag] = {}\n",
    "        if 'STOP' not in transition_dict['START'][tag]:\n",
    "            transition_dict['START'][tag]['STOP'] = sentence_w_one_word[tag] / len(input_list)\n",
    "            \n",
    "    return transition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'c': 0.375, 'STOP': 0.25},\n",
       " 'c': {'b': 0.25, 'a': 0.125},\n",
       " 'START': {'a': 0.25}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_word_sequences, training_tag_sequences = read_training_data(\"EN/train\")\n",
    "\n",
    "test = [['a','c','a','c'],['a','c','a','d'],['c','b','a','d','d'],['a','c','b'],['c','a','c'],['c','b','c'],['a'],['a']]\n",
    "\n",
    "create_transition_dict_second_order(test)['START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': {'O': {'O': 0.5,\n",
       "   'B-INTJ': 0.03289473684210526,\n",
       "   'B-PP': 0.019736842105263157,\n",
       "   'B-SBAR': 0.015350877192982455,\n",
       "   'B-NP': 0.2916666666666667,\n",
       "   'B-VP': 0.08771929824561403,\n",
       "   'B-ADVP': 0.041666666666666664,\n",
       "   'B-ADJP': 0.010964912280701754,\n",
       "   'STOP': 0.23049001814882034},\n",
       "  'B-INTJ': {'O': 0.4,\n",
       "   'I-INTJ': 0.31666666666666665,\n",
       "   'B-VP': 0.05,\n",
       "   'B-NP': 0.16666666666666666,\n",
       "   'B-ADVP': 0.016666666666666666,\n",
       "   'B-PP': 0.03333333333333333,\n",
       "   'B-INTJ': 0.016666666666666666,\n",
       "   'STOP': 0.029038112522686024},\n",
       "  'B-PP': {'B-NP': 0.6976744186046512,\n",
       "   'B-VP': 0.11627906976744186,\n",
       "   'O': 0.18604651162790697},\n",
       "  'B-NP': {'O': 0.09618320610687023,\n",
       "   'I-NP': 0.37251908396946565,\n",
       "   'B-VP': 0.4259541984732824,\n",
       "   'B-PP': 0.035114503816793895,\n",
       "   'B-SBAR': 0.0015267175572519084,\n",
       "   'B-NP': 0.02900763358778626,\n",
       "   'B-ADJP': 0.013740458015267175,\n",
       "   'B-ADVP': 0.024427480916030534,\n",
       "   'B-INTJ': 0.0015267175572519084,\n",
       "   'STOP': 0.007259528130671506},\n",
       "  'B-VP': {'B-PP': 0.13157894736842105,\n",
       "   'B-NP': 0.4517543859649123,\n",
       "   'B-PRT': 0.04824561403508772,\n",
       "   'I-VP': 0.20614035087719298,\n",
       "   'B-ADVP': 0.039473684210526314,\n",
       "   'O': 0.06140350877192982,\n",
       "   'B-INTJ': 0.008771929824561403,\n",
       "   'B-ADJP': 0.017543859649122806,\n",
       "   'B-SBAR': 0.017543859649122806,\n",
       "   'B-VP': 0.008771929824561403,\n",
       "   'I-INTJ': 0.0043859649122807015,\n",
       "   'I-NP': 0.0043859649122807015},\n",
       "  'B-SBAR': {'B-NP': 0.8571428571428571,\n",
       "   'O': 0.09523809523809523,\n",
       "   'I-SBAR': 0.047619047619047616},\n",
       "  'B-ADVP': {'B-VP': 0.2830188679245283,\n",
       "   'O': 0.1509433962264151,\n",
       "   'B-NP': 0.42452830188679247,\n",
       "   'B-SBAR': 0.009433962264150943,\n",
       "   'B-ADVP': 0.009433962264150943,\n",
       "   'I-ADVP': 0.08490566037735849,\n",
       "   'B-PP': 0.02830188679245283,\n",
       "   'I-INTJ': 0.009433962264150943},\n",
       "  'B-ADJP': {'B-PP': 0.11538461538461539,\n",
       "   'B-NP': 0.19230769230769232,\n",
       "   'O': 0.23076923076923078,\n",
       "   'I-ADJP': 0.4230769230769231,\n",
       "   'B-ADJP': 0.038461538461538464,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'I-NP': {'I-NP': 0.5, 'B-VP': 0.5},\n",
       "  'I-INTJ': {'O': 1.0},\n",
       "  'B-PRT': {'B-VP': 1.0}},\n",
       " 'B-INTJ': {'O': {'O': 0.1746031746031746,\n",
       "   'B-NP': 0.5079365079365079,\n",
       "   'B-VP': 0.047619047619047616,\n",
       "   'B-INTJ': 0.15873015873015872,\n",
       "   'B-ADVP': 0.1111111111111111,\n",
       "   'STOP': 0.019963702359346643},\n",
       "  'B-NP': {'I-NP': 0.2,\n",
       "   'O': 0.26666666666666666,\n",
       "   'B-VP': 0.4666666666666667,\n",
       "   'B-NP': 0.03333333333333333,\n",
       "   'B-ADVP': 0.03333333333333333,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'I-INTJ': {'I-INTJ': 0.2037037037037037,\n",
       "   'O': 0.5,\n",
       "   'B-INTJ': 0.05555555555555555,\n",
       "   'B-VP': 0.018518518518518517,\n",
       "   'B-NP': 0.18518518518518517,\n",
       "   'B-ADVP': 0.037037037037037035,\n",
       "   'STOP': 0.030852994555353903},\n",
       "  'B-VP': {'B-NP': 0.6, 'I-VP': 0.3, 'B-PP': 0.1},\n",
       "  'B-PP': {'B-NP': 1.0},\n",
       "  'B-ADVP': {'B-VP': 0.3333333333333333, 'B-NP': 0.6666666666666666},\n",
       "  'B-ADJP': {'I-ADJP': 0.5, 'O': 0.25, 'B-NP': 0.25},\n",
       "  'B-INTJ': {'I-INTJ': 0.3333333333333333,\n",
       "   'O': 0.2222222222222222,\n",
       "   'B-INTJ': 0.3333333333333333,\n",
       "   'B-VP': 0.1111111111111111,\n",
       "   'STOP': 0.0018148820326678765}},\n",
       " 'B-PP': {'B-NP': {'I-NP': 0.5758564437194127,\n",
       "   'B-PP': 0.05872756933115824,\n",
       "   'B-VP': 0.03915171288743882,\n",
       "   'O': 0.22838499184339314,\n",
       "   'B-ADVP': 0.022838499184339316,\n",
       "   'B-NP': 0.057096247960848286,\n",
       "   'B-INTJ': 0.0065252854812398045,\n",
       "   'B-SBAR': 0.009787928221859706,\n",
       "   'B-ADJP': 0.0016313213703099511,\n",
       "   'STOP': 0.018148820326678767},\n",
       "  'B-VP': {'B-ADJP': 0.041666666666666664,\n",
       "   'I-VP': 0.25,\n",
       "   'B-NP': 0.3333333333333333,\n",
       "   'B-ADVP': 0.08333333333333333,\n",
       "   'B-PP': 0.20833333333333334,\n",
       "   'B-PRT': 0.041666666666666664,\n",
       "   'O': 0.041666666666666664},\n",
       "  'O': {'O': 0.4358974358974359,\n",
       "   'B-PP': 0.07692307692307693,\n",
       "   'B-NP': 0.358974358974359,\n",
       "   'B-ADVP': 0.02564102564102564,\n",
       "   'B-VP': 0.07692307692307693,\n",
       "   'B-INTJ': 0.02564102564102564,\n",
       "   'STOP': 0.010889292196007259},\n",
       "  'B-ADVP': {'O': 0.25, 'B-VP': 0.25, 'I-ADVP': 0.25, 'B-NP': 0.25},\n",
       "  'B-SBAR': {'B-NP': 1.0},\n",
       "  'B-PP': {'B-NP': 0.75, 'O': 0.25},\n",
       "  'I-PP': {'I-PP': 0.25, 'B-NP': 0.75},\n",
       "  'B-INTJ': {'O': 1.0, 'STOP': 0.0018148820326678765},\n",
       "  'B-ADJP': {'B-VP': 0.5, 'O': 0.5}},\n",
       " 'B-NP': {'I-NP': {'O': 0.23605150214592274,\n",
       "   'B-PP': 0.13218884120171673,\n",
       "   'I-NP': 0.3725321888412017,\n",
       "   'B-NP': 0.07896995708154507,\n",
       "   'B-INTJ': 0.017167381974248927,\n",
       "   'B-VP': 0.12103004291845494,\n",
       "   'B-SBAR': 0.002575107296137339,\n",
       "   'B-CONJP': 0.0008583690987124463,\n",
       "   'B-ADVP': 0.030042918454935622,\n",
       "   'B-PRT': 0.002575107296137339,\n",
       "   'B-ADJP': 0.006008583690987125,\n",
       "   'STOP': 0.045372050816696916},\n",
       "  'O': {'B-NP': 0.4017094017094017,\n",
       "   'B-VP': 0.18233618233618235,\n",
       "   'O': 0.21082621082621084,\n",
       "   'B-INTJ': 0.10541310541310542,\n",
       "   'B-SBAR': 0.008547008547008548,\n",
       "   'B-PP': 0.03418803418803419,\n",
       "   'B-ADVP': 0.03133903133903134,\n",
       "   'B-ADJP': 0.022792022792022793,\n",
       "   'B-PRT': 0.002849002849002849,\n",
       "   'STOP': 0.14882032667876588},\n",
       "  'B-PP': {'B-NP': 0.8904109589041096,\n",
       "   'B-VP': 0.0273972602739726,\n",
       "   'O': 0.0410958904109589,\n",
       "   'B-ADVP': 0.0136986301369863,\n",
       "   'B-INTJ': 0.00684931506849315,\n",
       "   'B-ADJP': 0.00684931506849315,\n",
       "   'B-SBAR': 0.00684931506849315,\n",
       "   'I-PP': 0.00684931506849315},\n",
       "  'B-VP': {'B-PP': 0.06222222222222222,\n",
       "   'B-ADJP': 0.06666666666666667,\n",
       "   'B-NP': 0.3674074074074074,\n",
       "   'I-VP': 0.36,\n",
       "   'B-VP': 0.01037037037037037,\n",
       "   'B-ADVP': 0.05185185185185185,\n",
       "   'B-PRT': 0.022222222222222223,\n",
       "   'O': 0.044444444444444446,\n",
       "   'B-SBAR': 0.005925925925925926,\n",
       "   'B-INTJ': 0.005925925925925926,\n",
       "   'I-NP': 0.002962962962962963,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'B-NP': {'O': 0.2222222222222222,\n",
       "   'B-VP': 0.2986111111111111,\n",
       "   'B-ADVP': 0.013888888888888888,\n",
       "   'I-NP': 0.3541666666666667,\n",
       "   'B-INTJ': 0.006944444444444444,\n",
       "   'B-PP': 0.04861111111111111,\n",
       "   'B-NP': 0.041666666666666664,\n",
       "   'B-SBAR': 0.006944444444444444,\n",
       "   'B-ADJP': 0.006944444444444444,\n",
       "   'STOP': 0.009074410163339383},\n",
       "  'B-ADVP': {'B-VP': 0.36363636363636365,\n",
       "   'B-NP': 0.19480519480519481,\n",
       "   'O': 0.23376623376623376,\n",
       "   'I-ADVP': 0.09090909090909091,\n",
       "   'B-INTJ': 0.06493506493506493,\n",
       "   'B-ADVP': 0.025974025974025976,\n",
       "   'B-SBAR': 0.012987012987012988,\n",
       "   'B-PP': 0.012987012987012988,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-PRT': {'O': 0.5,\n",
       "   'B-PP': 0.25,\n",
       "   'B-NP': 0.16666666666666666,\n",
       "   'B-ADVP': 0.08333333333333333},\n",
       "  'B-INTJ': {'I-INTJ': 0.36363636363636365,\n",
       "   'O': 0.45454545454545453,\n",
       "   'B-INTJ': 0.18181818181818182,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'B-SBAR': {'I-SBAR': 0.16666666666666666, 'B-NP': 0.8333333333333334},\n",
       "  'B-ADJP': {'B-PP': 0.3181818181818182,\n",
       "   'O': 0.13636363636363635,\n",
       "   'I-ADJP': 0.3181818181818182,\n",
       "   'B-ADVP': 0.09090909090909091,\n",
       "   'B-INTJ': 0.045454545454545456,\n",
       "   'B-NP': 0.045454545454545456,\n",
       "   'B-VP': 0.045454545454545456,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'I-INTJ': {'STOP': 0.0018148820326678765}},\n",
       " 'I-NP': {'O': {'B-NP': 0.4255874673629243,\n",
       "   'B-INTJ': 0.0391644908616188,\n",
       "   'B-VP': 0.15404699738903394,\n",
       "   'B-PP': 0.031331592689295036,\n",
       "   'O': 0.27154046997389036,\n",
       "   'B-ADVP': 0.05483028720626632,\n",
       "   'B-ADJP': 0.015665796344647518,\n",
       "   'B-SBAR': 0.0026109660574412533,\n",
       "   'I-NP': 0.0026109660574412533,\n",
       "   'I-INTJ': 0.0026109660574412533,\n",
       "   'STOP': 0.15970961887477314},\n",
       "  'B-PP': {'B-NP': 0.8728813559322034,\n",
       "   'O': 0.07203389830508475,\n",
       "   'B-VP': 0.03389830508474576,\n",
       "   'B-SBAR': 0.00423728813559322,\n",
       "   'B-ADVP': 0.00423728813559322,\n",
       "   'B-PP': 0.00423728813559322,\n",
       "   'B-ADJP': 0.00423728813559322,\n",
       "   'B-INTJ': 0.00423728813559322},\n",
       "  'I-NP': {'I-NP': 0.39136690647482014,\n",
       "   'O': 0.2820143884892086,\n",
       "   'B-NP': 0.07625899280575539,\n",
       "   'B-PP': 0.11510791366906475,\n",
       "   'B-VP': 0.08920863309352518,\n",
       "   'B-SBAR': 0.0057553956834532375,\n",
       "   'B-ADVP': 0.02158273381294964,\n",
       "   'B-INTJ': 0.015827338129496403,\n",
       "   'I-PP': 0.0014388489208633094,\n",
       "   'B-PRT': 0.0014388489208633094,\n",
       "   'STOP': 0.023593466424682397},\n",
       "  'B-NP': {'B-PP': 0.07746478873239436,\n",
       "   'O': 0.2746478873239437,\n",
       "   'B-VP': 0.24647887323943662,\n",
       "   'I-NP': 0.352112676056338,\n",
       "   'I-INTJ': 0.007042253521126761,\n",
       "   'B-NP': 0.02112676056338028,\n",
       "   'B-ADVP': 0.007042253521126761,\n",
       "   'B-ADJP': 0.007042253521126761,\n",
       "   'B-SBAR': 0.007042253521126761,\n",
       "   'STOP': 0.007259528130671506},\n",
       "  'B-INTJ': {'B-NP': 0.19047619047619047,\n",
       "   'I-INTJ': 0.3333333333333333,\n",
       "   'O': 0.38095238095238093,\n",
       "   'B-PP': 0.047619047619047616,\n",
       "   'B-ADJP': 0.047619047619047616,\n",
       "   'STOP': 0.018148820326678767},\n",
       "  'B-VP': {'B-ADJP': 0.08333333333333333,\n",
       "   'I-VP': 0.3382352941176471,\n",
       "   'B-NP': 0.3088235294117647,\n",
       "   'B-PP': 0.11764705882352941,\n",
       "   'B-PRT': 0.029411764705882353,\n",
       "   'O': 0.06372549019607843,\n",
       "   'B-ADVP': 0.05392156862745098,\n",
       "   'B-VP': 0.004901960784313725},\n",
       "  'B-SBAR': {'B-ADVP': 0.14285714285714285, 'B-NP': 0.8571428571428571},\n",
       "  'B-CONJP': {'I-CONJP': 1.0},\n",
       "  'B-ADVP': {'O': 0.3877551020408163,\n",
       "   'B-VP': 0.22448979591836735,\n",
       "   'I-ADVP': 0.14285714285714285,\n",
       "   'B-NP': 0.14285714285714285,\n",
       "   'B-ADVP': 0.061224489795918366,\n",
       "   'B-PP': 0.02040816326530612,\n",
       "   'B-INTJ': 0.02040816326530612,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-PRT': {'B-ADVP': 0.3333333333333333,\n",
       "   'O': 0.3333333333333333,\n",
       "   'B-NP': 0.3333333333333333,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-ADJP': {'O': 0.2857142857142857,\n",
       "   'B-PP': 0.14285714285714285,\n",
       "   'B-INTJ': 0.14285714285714285,\n",
       "   'I-ADJP': 0.42857142857142855},\n",
       "  'I-PP': {'B-NP': 1.0}},\n",
       " 'B-VP': {'B-PRT': {'B-NP': 0.4186046511627907,\n",
       "   'B-PP': 0.27906976744186046,\n",
       "   'B-INTJ': 0.06976744186046512,\n",
       "   'O': 0.16279069767441862,\n",
       "   'B-ADVP': 0.046511627906976744,\n",
       "   'B-VP': 0.023255813953488372},\n",
       "  'B-PP': {'B-NP': 0.9067796610169492,\n",
       "   'O': 0.0423728813559322,\n",
       "   'B-ADVP': 0.00847457627118644,\n",
       "   'I-PP': 0.01694915254237288,\n",
       "   'B-VP': 0.01694915254237288,\n",
       "   'B-PP': 0.00847457627118644,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'I-VP': {'B-PP': 0.10922330097087378,\n",
       "   'I-VP': 0.3762135922330097,\n",
       "   'B-ADJP': 0.03398058252427184,\n",
       "   'O': 0.08009708737864078,\n",
       "   'B-NP': 0.30339805825242716,\n",
       "   'B-INTJ': 0.0048543689320388345,\n",
       "   'B-ADVP': 0.038834951456310676,\n",
       "   'B-PRT': 0.03398058252427184,\n",
       "   'B-VP': 0.009708737864077669,\n",
       "   'B-SBAR': 0.009708737864077669,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'B-NP': {'I-NP': 0.45875251509054327,\n",
       "   'B-PP': 0.08048289738430583,\n",
       "   'B-NP': 0.10261569416498995,\n",
       "   'O': 0.16096579476861167,\n",
       "   'B-VP': 0.12877263581488935,\n",
       "   'B-PRT': 0.018108651911468814,\n",
       "   'B-ADVP': 0.03219315895372234,\n",
       "   'B-ADJP': 0.008048289738430584,\n",
       "   'B-INTJ': 0.006036217303822937,\n",
       "   'B-SBAR': 0.004024144869215292,\n",
       "   'STOP': 0.009074410163339383},\n",
       "  'B-ADJP': {'B-INTJ': 0.029850746268656716,\n",
       "   'B-PP': 0.208955223880597,\n",
       "   'O': 0.26865671641791045,\n",
       "   'I-ADJP': 0.31343283582089554,\n",
       "   'B-VP': 0.07462686567164178,\n",
       "   'B-SBAR': 0.029850746268656716,\n",
       "   'I-NP': 0.014925373134328358,\n",
       "   'B-NP': 0.05970149253731343,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'B-PRT': 0.2727272727272727,\n",
       "   'I-VP': 0.18181818181818182,\n",
       "   'O': 0.09090909090909091,\n",
       "   'B-NP': 0.36363636363636365,\n",
       "   'B-PP': 0.09090909090909091},\n",
       "  'B-ADVP': {'B-NP': 0.4461538461538462,\n",
       "   'O': 0.2153846153846154,\n",
       "   'B-ADJP': 0.015384615384615385,\n",
       "   'I-ADVP': 0.1076923076923077,\n",
       "   'B-PP': 0.13846153846153847,\n",
       "   'B-ADVP': 0.015384615384615385,\n",
       "   'B-VP': 0.06153846153846154,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'O': {'O': 0.19117647058823528,\n",
       "   'B-NP': 0.4411764705882353,\n",
       "   'B-ADVP': 0.1323529411764706,\n",
       "   'B-VP': 0.17647058823529413,\n",
       "   'B-PP': 0.029411764705882353,\n",
       "   'B-INTJ': 0.014705882352941176,\n",
       "   'B-SBAR': 0.014705882352941176,\n",
       "   'STOP': 0.019963702359346643},\n",
       "  'B-INTJ': {'B-VP': 0.4,\n",
       "   'B-INTJ': 0.2,\n",
       "   'O': 0.2,\n",
       "   'I-INTJ': 0.2,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-SBAR': {'B-NP': 0.9166666666666666, 'O': 0.08333333333333333},\n",
       "  'I-NP': {'B-PP': 0.6666666666666666, 'I-NP': 0.3333333333333333},\n",
       "  'I-INTJ': {'STOP': 0.0018148820326678765}},\n",
       " 'B-PRT': {'B-NP': {'O': 0.2413793103448276,\n",
       "   'I-NP': 0.6896551724137931,\n",
       "   'B-VP': 0.034482758620689655,\n",
       "   'B-PP': 0.034482758620689655,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'O': {'O': 0.2857142857142857,\n",
       "   'B-NP': 0.21428571428571427,\n",
       "   'B-VP': 0.14285714285714285,\n",
       "   'B-ADVP': 0.21428571428571427,\n",
       "   'B-INTJ': 0.07142857142857142,\n",
       "   'B-SBAR': 0.07142857142857142,\n",
       "   'STOP': 0.009074410163339383},\n",
       "  'B-PP': {'B-NP': 0.8695652173913043, 'O': 0.13043478260869565},\n",
       "  'B-ADVP': {'I-ADVP': 0.6, 'B-ADVP': 0.2, 'O': 0.2},\n",
       "  'B-INTJ': {'B-ADJP': 1.0, 'STOP': 0.003629764065335753},\n",
       "  'B-VP': {'I-VP': 0.6666666666666666, 'B-NP': 0.3333333333333333}},\n",
       " 'I-VP': {'B-PP': {'B-NP': 0.8904109589041096,\n",
       "   'O': 0.0410958904109589,\n",
       "   'B-VP': 0.0273972602739726,\n",
       "   'B-PP': 0.0273972602739726,\n",
       "   'B-ADVP': 0.0136986301369863},\n",
       "  'I-VP': {'O': 0.07936507936507936,\n",
       "   'I-VP': 0.20105820105820105,\n",
       "   'B-PP': 0.14814814814814814,\n",
       "   'B-NP': 0.4074074074074074,\n",
       "   'B-ADVP': 0.05291005291005291,\n",
       "   'B-PRT': 0.0582010582010582,\n",
       "   'B-INTJ': 0.010582010582010581,\n",
       "   'B-VP': 0.005291005291005291,\n",
       "   'B-SBAR': 0.015873015873015872,\n",
       "   'B-ADJP': 0.021164021164021163,\n",
       "   'STOP': 0.007259528130671506},\n",
       "  'O': {'B-VP': 0.21052631578947367,\n",
       "   'B-NP': 0.2894736842105263,\n",
       "   'B-INTJ': 0.18421052631578946,\n",
       "   'O': 0.13157894736842105,\n",
       "   'B-PP': 0.02631578947368421,\n",
       "   'B-ADVP': 0.07894736842105263,\n",
       "   'I-NP': 0.02631578947368421,\n",
       "   'B-SBAR': 0.05263157894736842,\n",
       "   'STOP': 0.018148820326678767},\n",
       "  'B-ADJP': {'B-VP': 0.05555555555555555,\n",
       "   'I-ADJP': 0.3333333333333333,\n",
       "   'B-SBAR': 0.05555555555555555,\n",
       "   'B-PP': 0.16666666666666666,\n",
       "   'B-INTJ': 0.1111111111111111,\n",
       "   'O': 0.2777777777777778},\n",
       "  'B-NP': {'I-NP': 0.48258706467661694,\n",
       "   'B-PP': 0.08955223880597014,\n",
       "   'O': 0.12437810945273632,\n",
       "   'B-NP': 0.07960199004975124,\n",
       "   'B-VP': 0.10945273631840796,\n",
       "   'B-ADVP': 0.05970149253731343,\n",
       "   'B-SBAR': 0.004975124378109453,\n",
       "   'B-PRT': 0.014925373134328358,\n",
       "   'B-ADJP': 0.014925373134328358,\n",
       "   'B-INTJ': 0.01990049751243781,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-ADVP': {'B-VP': 0.08,\n",
       "   'I-ADVP': 0.16,\n",
       "   'O': 0.36,\n",
       "   'B-NP': 0.2,\n",
       "   'B-ADJP': 0.04,\n",
       "   'B-PP': 0.12,\n",
       "   'B-INTJ': 0.04,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-PRT': {'B-PP': 0.3333333333333333,\n",
       "   'O': 0.20833333333333334,\n",
       "   'B-NP': 0.375,\n",
       "   'B-VP': 0.041666666666666664,\n",
       "   'B-ADVP': 0.041666666666666664,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'B-NP': 0.75, 'B-ADVP': 0.25, 'STOP': 0.0018148820326678765},\n",
       "  'B-INTJ': {'B-INTJ': 1.0, 'STOP': 0.0054446460980036296},\n",
       "  'B-SBAR': {'B-NP': 0.8571428571428571, 'O': 0.14285714285714285}},\n",
       " 'B-ADJP': {'B-PP': {'B-NP': 0.9285714285714286,\n",
       "   'B-ADVP': 0.03571428571428571,\n",
       "   'B-VP': 0.03571428571428571,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'I-VP': 0.75, 'B-VP': 0.125, 'B-NP': 0.125},\n",
       "  'I-ADJP': {'O': 0.4,\n",
       "   'B-NP': 0.12,\n",
       "   'B-PP': 0.14,\n",
       "   'B-INTJ': 0.04,\n",
       "   'B-SBAR': 0.02,\n",
       "   'B-ADVP': 0.1,\n",
       "   'I-ADJP': 0.16,\n",
       "   'B-VP': 0.02,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'O': {'O': 0.10714285714285714,\n",
       "   'B-VP': 0.17857142857142858,\n",
       "   'B-NP': 0.35714285714285715,\n",
       "   'B-INTJ': 0.10714285714285714,\n",
       "   'B-ADVP': 0.14285714285714285,\n",
       "   'B-ADJP': 0.07142857142857142,\n",
       "   'B-SBAR': 0.03571428571428571,\n",
       "   'STOP': 0.019963702359346643},\n",
       "  'B-SBAR': {'B-NP': 1.0},\n",
       "  'B-NP': {'B-VP': 0.3333333333333333,\n",
       "   'I-NP': 0.3333333333333333,\n",
       "   'O': 0.25,\n",
       "   'B-NP': 0.08333333333333333},\n",
       "  'B-ADVP': {'O': 0.6666666666666666, 'B-NP': 0.3333333333333333},\n",
       "  'I-NP': {'B-NP': 1.0},\n",
       "  'B-INTJ': {'O': 0.3333333333333333,\n",
       "   'B-ADVP': 0.6666666666666666,\n",
       "   'STOP': 0.0054446460980036296},\n",
       "  'B-ADJP': {'B-ADJP': 0.5, 'O': 0.5}},\n",
       " 'B-SBAR': {'B-NP': {'B-VP': 0.6885245901639344,\n",
       "   'I-NP': 0.19672131147540983,\n",
       "   'O': 0.04918032786885246,\n",
       "   'B-NP': 0.04918032786885246,\n",
       "   'B-PP': 0.01639344262295082},\n",
       "  'B-ADVP': {'B-NP': 1.0},\n",
       "  'O': {'B-NP': 0.2, 'B-INTJ': 0.2, 'O': 0.2, 'B-VP': 0.4},\n",
       "  'I-SBAR': {'B-NP': 1.0}},\n",
       " 'B-ADVP': {'B-NP': {'B-ADVP': 0.072,\n",
       "   'B-VP': 0.552,\n",
       "   'I-NP': 0.24,\n",
       "   'O': 0.072,\n",
       "   'B-PP': 0.008,\n",
       "   'B-ADJP': 0.008,\n",
       "   'B-NP': 0.048,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'I-VP': 0.22727272727272727,\n",
       "   'B-NP': 0.42045454545454547,\n",
       "   'B-PRT': 0.03409090909090909,\n",
       "   'B-PP': 0.09090909090909091,\n",
       "   'O': 0.1590909090909091,\n",
       "   'B-SBAR': 0.03409090909090909,\n",
       "   'B-ADVP': 0.03409090909090909},\n",
       "  'O': {'O': 0.16901408450704225,\n",
       "   'B-NP': 0.4084507042253521,\n",
       "   'B-ADVP': 0.14084507042253522,\n",
       "   'B-INTJ': 0.07042253521126761,\n",
       "   'B-VP': 0.1267605633802817,\n",
       "   'B-PP': 0.028169014084507043,\n",
       "   'B-SBAR': 0.014084507042253521,\n",
       "   'B-ADJP': 0.04225352112676056,\n",
       "   'STOP': 0.023593466424682397},\n",
       "  'I-ADVP': {'O': 0.5,\n",
       "   'B-NP': 0.14285714285714285,\n",
       "   'B-INTJ': 0.047619047619047616,\n",
       "   'I-ADVP': 0.16666666666666666,\n",
       "   'B-VP': 0.047619047619047616,\n",
       "   'B-ADJP': 0.023809523809523808,\n",
       "   'B-SBAR': 0.047619047619047616,\n",
       "   'B-PP': 0.023809523809523808,\n",
       "   'STOP': 0.003629764065335753},\n",
       "  'B-INTJ': {'I-INTJ': 0.4,\n",
       "   'B-INTJ': 0.2,\n",
       "   'B-NP': 0.2,\n",
       "   'O': 0.2,\n",
       "   'STOP': 0.0054446460980036296},\n",
       "  'B-PP': {'B-NP': 0.8947368421052632,\n",
       "   'O': 0.05263157894736842,\n",
       "   'B-VP': 0.05263157894736842},\n",
       "  'B-ADVP': {'B-NP': 0.875, 'O': 0.125},\n",
       "  'B-SBAR': {'I-SBAR': 0.5, 'B-NP': 0.5},\n",
       "  'B-ADJP': {'B-PP': 1.0, 'STOP': 0.0018148820326678765},\n",
       "  'I-INTJ': {'STOP': 0.0018148820326678765}},\n",
       " 'B-CONJP': {'I-CONJP': {'B-VP': 1.0}},\n",
       " 'I-CONJP': {'B-VP': {'I-VP': 0.5, 'B-NP': 0.5}},\n",
       " 'I-INTJ': {'I-INTJ': {'B-VP': 0.1111111111111111,\n",
       "   'B-INTJ': 0.1111111111111111,\n",
       "   'I-INTJ': 0.2222222222222222,\n",
       "   'O': 0.4444444444444444,\n",
       "   'B-ADVP': 0.1111111111111111,\n",
       "   'STOP': 0.007259528130671506},\n",
       "  'B-VP': {'B-NP': 0.5, 'I-VP': 0.5},\n",
       "  'O': {'B-NP': 0.4230769230769231,\n",
       "   'B-VP': 0.19230769230769232,\n",
       "   'B-INTJ': 0.038461538461538464,\n",
       "   'B-ADVP': 0.15384615384615385,\n",
       "   'O': 0.19230769230769232,\n",
       "   'STOP': 0.010889292196007259},\n",
       "  'B-INTJ': {'I-INTJ': 1.0, 'STOP': 0.003629764065335753},\n",
       "  'B-NP': {'O': 0.2, 'B-VP': 0.6, 'B-NP': 0.1, 'I-NP': 0.1},\n",
       "  'B-ADVP': {'B-NP': 1.0}},\n",
       " 'I-ADJP': {'O': {'O': 0.26666666666666666,\n",
       "   'B-VP': 0.06666666666666667,\n",
       "   'B-ADVP': 0.06666666666666667,\n",
       "   'B-NP': 0.4,\n",
       "   'B-INTJ': 0.13333333333333333,\n",
       "   'B-SBAR': 0.06666666666666667,\n",
       "   'STOP': 0.014519056261343012},\n",
       "  'B-NP': {'B-VP': 0.16666666666666666,\n",
       "   'O': 0.6666666666666666,\n",
       "   'I-NP': 0.16666666666666666,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-PP': {'B-NP': 0.6666666666666666,\n",
       "   'I-PP': 0.1111111111111111,\n",
       "   'B-VP': 0.1111111111111111,\n",
       "   'O': 0.1111111111111111},\n",
       "  'B-INTJ': {'I-INTJ': 1.0, 'STOP': 0.0018148820326678765},\n",
       "  'B-SBAR': {'B-NP': 1.0},\n",
       "  'B-ADVP': {'O': 0.2, 'B-NP': 0.4, 'I-ADVP': 0.2, 'B-PP': 0.2},\n",
       "  'I-ADJP': {'B-PP': 0.25,\n",
       "   'O': 0.375,\n",
       "   'B-NP': 0.125,\n",
       "   'B-VP': 0.125,\n",
       "   'I-ADJP': 0.125,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'I-VP': 0.5, 'O': 0.5}},\n",
       " 'I-ADVP': {'O': {'O': 0.21052631578947367,\n",
       "   'B-NP': 0.42105263157894735,\n",
       "   'B-PP': 0.10526315789473684,\n",
       "   'B-INTJ': 0.10526315789473684,\n",
       "   'B-ADVP': 0.10526315789473684,\n",
       "   'B-VP': 0.05263157894736842,\n",
       "   'STOP': 0.009074410163339383},\n",
       "  'B-NP': {'B-ADVP': 0.1111111111111111,\n",
       "   'I-NP': 0.4444444444444444,\n",
       "   'B-VP': 0.2222222222222222,\n",
       "   'O': 0.1111111111111111,\n",
       "   'B-PP': 0.1111111111111111},\n",
       "  'B-INTJ': {'B-ADVP': 0.5, 'I-INTJ': 0.5},\n",
       "  'I-ADVP': {'O': 0.42857142857142855,\n",
       "   'B-PP': 0.14285714285714285,\n",
       "   'B-NP': 0.42857142857142855},\n",
       "  'B-VP': {'B-ADJP': 0.5, 'B-NP': 0.5},\n",
       "  'B-PP': {'B-ADVP': 0.5, 'B-NP': 0.5},\n",
       "  'B-ADJP': {'B-NP': 1.0},\n",
       "  'B-SBAR': {'O': 0.5, 'B-NP': 0.5}},\n",
       " 'I-SBAR': {'B-NP': {'B-NP': 0.5, 'B-VP': 0.5}},\n",
       " 'I-PP': {'I-PP': {'B-NP': 1.0}, 'B-NP': {'I-NP': 0.6, 'O': 0.4}},\n",
       " 'START': {'O': {'O': 0.17785843920145192,\n",
       "   'B-NP': 0.12159709618874773,\n",
       "   'B-ADVP': 0.019963702359346643,\n",
       "   'B-INTJ': 0.06352087114337568,\n",
       "   'B-VP': 0.025408348457350273,\n",
       "   'B-ADJP': 0.0054446460980036296,\n",
       "   'B-SBAR': 0.0054446460980036296,\n",
       "   'STOP': 0.0018148820326678765},\n",
       "  'B-VP': {'B-PRT': 0.007259528130671506,\n",
       "   'I-VP': 0.023593466424682397,\n",
       "   'B-ADVP': 0.009074410163339383,\n",
       "   'B-NP': 0.045372050816696916,\n",
       "   'B-PP': 0.014519056261343012,\n",
       "   'B-SBAR': 0.0018148820326678765,\n",
       "   'O': 0.009074410163339383},\n",
       "  'B-NP': {'I-NP': 0.1560798548094374,\n",
       "   'B-VP': 0.12522686025408347,\n",
       "   'O': 0.02722323049001815,\n",
       "   'B-ADVP': 0.010889292196007259,\n",
       "   'B-PP': 0.012704174228675136,\n",
       "   'B-NP': 0.009074410163339383,\n",
       "   'B-ADJP': 0.0054446460980036296},\n",
       "  'B-INTJ': {'O': 0.012704174228675136,\n",
       "   'B-NP': 0.012704174228675136,\n",
       "   'I-INTJ': 0.021778584392014518,\n",
       "   'B-VP': 0.0018148820326678765,\n",
       "   'B-ADJP': 0.003629764065335753,\n",
       "   'B-ADVP': 0.0018148820326678765},\n",
       "  'B-ADVP': {'B-VP': 0.016333938294010888,\n",
       "   'I-ADVP': 0.007259528130671506,\n",
       "   'B-NP': 0.009074410163339383,\n",
       "   'O': 0.0018148820326678765,\n",
       "   'B-PP': 0.0018148820326678765,\n",
       "   'B-INTJ': 0.0018148820326678765},\n",
       "  'B-PP': {'B-NP': 0.012704174228675136, 'B-ADVP': 0.0018148820326678765},\n",
       "  'B-ADJP': {'I-ADJP': 0.003629764065335753,\n",
       "   'B-ADVP': 0.0018148820326678765,\n",
       "   'O': 0.003629764065335753},\n",
       "  'B-SBAR': {'B-NP': 0.003629764065335753},\n",
       "  'B-CONJP': {'I-CONJP': 0.0018148820326678765},\n",
       "  'START': {'O': 0.0018148820326678765}}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_word_sequences, training_tag_sequences = read_training_data(\"EN/train\")\n",
    "create_transition_dict_second_order(training_tag_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
