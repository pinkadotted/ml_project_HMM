{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning\n",
    "## Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Report the precision, recall and F scores of such a baseline system for each dataset:\n",
    "- EN dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.5348\n",
    "    - Entity  recall: 0.7656\n",
    "    - Entity  F: 0.6297\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.3902\n",
    "    - Sentiment  recall: 0.5586\n",
    "    - Sentiment  F: 0.4595\n",
    "- FR dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.1670\n",
    "    - Entity  recall: 0.7815\n",
    "    - Entity  F: 0.2751\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.0709\n",
    "    - Sentiment  recall: 0.3319\n",
    "    - Sentiment  F: 0.1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['word1', 'word1', 'word1', 'word1'], ['word2', 'word2', 'word2'], ['word3']]\n",
      "[['tag1', 'tag1', 'tag1', 'tag2'], ['tag2', 'tag2', 'tag2'], ['tag3']]\n",
      "[['NO', 'Saints', 'R', '.', 'Buch', 'might', 'come', 'back', 'n', 'play', 'vs', 'Seahawks', 'on', 'Sunday', '??'], ['Polling', 'ends', 'in', 'Bihar', 'today', ',', 'counting', 'on', 'November', '24', 'http://toi.in/ujtwya'], ['Today', 'has', 'been', 'a', 'GREAT', 'day', '__AND__', 'it', 'just', 'keeps', 'getting', 'better', '!!!', 'Thank', 'You', 'Lord', '!'], ['man', 'my', 'twin', 'wanna', 'act', 'fake', 'today', 'but', 'its', 'okay', 'bcuz', 'i', 'still', 'love', 'you'], ['@iians', 'Id', 'love', 'to', 'see', 'IDWT', 'live', 'from', 'CES', '.', 'And', 'one', 'day', 'I', 'hope', 'to', 'go', 'to', 'ces', 'myself', '.'], ['@Team_Panic', 'omg', ',', 'i', 'wish', 'i', 'got', 'paid', 'to', 'freak', 'people', 'out', '.', 'it', \"'s\", 'so', 'funny', 'when', 'they', 'think', \"you'r\", 'serious', '.', 'noobs', '.'], ['fckn', 'emily', 'rodriguez', 'did', \"n't\", 'bring', 'my', 'cookies', 'today', '!!', 'lil', 'brat', '.'], ['Amazon', 'U.K.', 'Offering', 'HTC', 'Desire', 'Z', 'Unlocked', 'October', '11', ':', 'We', 'just', 'got', 'official', 'word', 'of', 'the', 'HTC', 'Desire', 'Z', 'earlier', 'in', 'Lo', '...', 'http://bit.ly/bsyz9H'], ['Lmao', '..', 'They', '#Hurrt', '!', 'RT', '@_sofucKENNrude', 'y', 'every', 'nigga', 'tryna', 'grow', 'they', 'hair', 'out', '.', 'should', 'of', 'did', 'that', 'a', 'long', 'time', 'ago', '.', '-_-', 'skimp', 'ass', 'braids', '.'], ['@', 'home', 'chilling', 'guess', 'I', \"'m\", 'staying', 'in', '2day', '2', 'rest', 'this', 'week', 'was', 'crazy', 'but', 'I', \"'m\", 'ready', 'for', 'dis', 'weekend'], ['On', 'Thanksgiving', 'after', 'you', 'done', 'eating', 'its', '#TimeToGetOut', 'unless', 'you', 'wanna', 'help', 'with', 'the', 'dishes'], ['Got', 'another', 'two', 'box', 'of', 'chocolate', 'from', 'sister', '.', '.', 'Rewind', 'the', '1st', 'time', 'I', 'gave', 'you', 'a', 'bar', 'of', 'chocolate', '.', '.', 'Everything', 'pratically', 'remind', 'me', 'of', 'you', '?'], ['tomorrow', 'no', 'school', 'for', 'me', 'but', 'I', \"'m\", 'so', 'anxious', 'beacuse', 'of', 'tomorrow', 'omg', ':/'], ['PREACH', '!!', 'RT', '@DJRyan1der', ':', 'These', 'ppl', 'do', \"n't\", 'understand', 'its', 'Monday', 'and', 'I', \"'m\", 'not', 'in', 'da', 'mood', 'to', 'deal', 'wit', 'their', 'bullshit', '!!', 'Fukk', 'outta', 'here', '!!!'], ['The', 'guys', 'are', 'cooking', 'about', 'a', 'dozen', 'pizzas', 'at', 'our', 'house', 'for', 'the', 'tailgate', 'tonight', '.', 'Nonetheless', 'it', 'smells', 'fresh', 'in', 'this', 'place', '.'], ['Think', \"I'ma\", 'make', '@GrownWomanStuff', 'have', 'a', 'shoegasm', 'today', 'hahahaha'], ['even', 'better', 'than', 'cocktail', 'friday', '!', 'http://is.gd/ffDb9'], ['I', 'kept', 'telling', 'niggers', \"it'd\", 'happen', '.', 'Come', 'Thursday', 'im', 'inna', 'boardroom', 'full', 'of', 'niggers', '3times', 'my', 'age', '.', '.', '.', 'at', 'the', 'tender', 'age', 'of', '17', '!!', 'Im', 'pleased', '.'], ['Shower', 'time', 'the', 'flyin', 'pilot', 'I', 'was', 'at', 'last', 'night', 'did', \"n't\", 'have', 'no', 'hot', 'water', 'uggghh'], ['@hime8644', 'LOL', 'theres', 'always', 'some', 'good', 'vids', 'I', 'didnt', 'see', 'before', 'like', 'this', 'one', 'XD', '---', '&gt;', 'http://www.youtube.com/watch?v=8zJKZJ7TGlU', 'OMG', '*DROOLS*'], ['It', 'is', 'Friday', 'afternoon', 'adn', 'I', 'could', \"n't\", 'be', 'happier', '.', 'Ending', 'a', 'crappy', 'week', 'on', 'a', 'good', 'note', 'is', 'always', 'nice', '.'], ['LADY', 'GAGA', 'IS', 'BETTER', 'THE', '5th', 'TIME', 'OH', 'BABY(', ':'], ['Pope', 'says', 'religion', \"'\", \"marginalized':\", 'That', \"'s\", 'why', 'he', 'ca', \"n't\", 'get', 'a', 'headline', 'printed', 'anywhere', ',', 'because', 'religion', 'is', 'so', 'mar', '...', 'http://bit.ly/cBvt1u'], ['@ashhleybrookee', 'it', 'may', 'be', 'nasty', 'but', '...', 'it', 'works', '.'], ['So', 'this', 'will', 'be', 'more', 'lhs', 'games', 'then', 'i', 'went', 'to', 'then', 'when', 'i', 'went', 'to', 'lhs'], ['RT', '@DamnTeenQuotes', ':', 'I', 'remember', 'when', 'i', 'was', 'your', 'age', ',', 'spencer', 'from', 'iCarly', 'was', 'Crazy', 'Steve', ',', 'Carly', 'was', 'Megan', 'and', 'Josh', 'was', 'fat', '.', '#damnteenquotes'], ['[', 'GigaOM', ']', 'DRM', 'FAIL', ':', 'Five', 'Broken', 'Copy', 'Protection', 'Schemes', ':', 'This', 'week', ',', 'we', 'learned', 'that', 'the', 'HDCP', 'copy', 'protection', 'scheme', '...', 'http://bit.ly/blWf0e'], ['Friday', 'Night', 'Eats', 'http://twitpic.com/2pdvtr'], ['I', 'remember', 'when', 'you', 'all', 'were', 'thiiis', 'big', ',', 'you', 'know', '?'], ['RT', '@PeepsMagoo', ':', 'Please', 'help', 'me', 'save', 'Sunshine', '.', 'She', 'is', 'scheduled', 'to', 'be', 'put', 'down', 'at', 'a', 'Camarillo', 'shelter', 'on', 'Jan', '6', 'http://fragurl.com/5P'], ['I', 'have', 'a', 'strong', 'hunch', 'Adam', 'will', 'be', 'a', 'story', 'on', 'TMZ', 'tonight', '.', 'Gulp', '.'], ['WIN', 'FREE', 'PAMPERS', 'FOR', 'A', 'YEAR', '!!!', 'And', 'more', '...', 'http://nblo.gs/80qJu'], ['holy', 'FUCK', 'THIS', 'IS', 'THE', 'MOST', 'BORING', 'DAY', 'EVER', '.'], ['A', 'TRUE', 'friend', 'walks', 'in', 'when', 'everybody', 'else', 'walks', 'out', '!'], ['#Astros', 'lineup', 'for', 'tonight', '.', 'Keppinger', 'sits', ',', 'Downs', 'plays', '2B', ',', 'CJ', 'bats', '5th', '.', '@alysonfooter', 'http://bit.ly/bHvgCS'], ['RT', '@rockerfuckerak', ':', 'What', \"'s\", 'done', 'is', 'done', ',', 'Just', 'leave', 'it', 'alone', ',', 'and', 'do', \"n't\", 'regret', '.', 'Sometimes', 'somethings', 'turn', 'in-to', 'dumb', 'things', '.', 'And', 'that', \"'s\", 'when', '...'], ['Long', '18k', 'run', 'done', 'and', 'in', 'the', 'books', '!', 'Beauty', 'day', '.', 'Til', 'now', ',', 'at', 'dentist', ',', 'and', 'lady', 'next', 'to', 'me', 'spilling', 'over', 'to', 'my', 'chair', '..', 'Just', 'belched', '.', 'Loudly', '.', '+1'], ['RT', '@JupiterStorm', ':', '1', '.', '6', '.', '2011', '.', 'The', 'day', '@owlcity', 'went', 'on', 'an', 'Emporer', \"'s\", 'New', 'Groove', 'rampage', 'through', 'twitter', '.', 'xD'], ['hows', 'everyones', 'day', 'goin', '?'], ['@PERSONGUY6661', 'I', 'have', \"n't\", 'unfortunately', ':(', 'I', 'missed', 'the', 'chance', 'when', 'theycame', 'to', 'the', 'uk', 'this', 'year', ',', 'you', 'seen', 'them', '?'], ['@Yeweezii', 'Jst', 'lock', 'her', 'outside', '4', 'd', 'nite', '.', 'It', 'workz', 'al', 'd', 'time', ':-D'], ['@keancipriano', 'hey', ':)', 'where', 'exactly', 'is', 'your', 'gig', 'tonight', '?', ':)'], ['@YoDayDay', 'Nope', '!', '*lil', 'kid', 'voice*', 'im', 'being', 'stingy', 'tonight', '!!', '*hmph*'], ['@Phoebe1_', 'and', 'i', 'also', 'loved', 'the', 'last', 'years', 'eurovision', 'entry', '!!', 'hoppaa', '!', 'they', 'were', 'cool', 'too', '!!'], ['\"', '@DJDROC', ':', '#beforeserato', 'djs', 'actually', 'used', 'headphones', 'believe', 'it', 'or', 'not', '\"', '---', 'except', 'majestic', 'and', 'me', 'when', 'we', 'were', 'drunk', '...', 'haha'], ['RT', '@robmoysey', 'Eyeopener', 'vs', '.', 'Ryerson', 'Quidditch', 'team', 'this', 'Sunday', 'at', '4', 'p.m.', 'Anyone', 'know', 'where', 'to', 'get', 'cheap', 'brooms', '?', '#Ryerson', '@RUQuidditch', '#Rams'], ['@yoopergirl89', 'Yeah', 'it', 'was', 'a', 'long', 'week', 'here', 'too', '.', 'Luckily', 'for', 'me', 'next', 'is', 'only', 'Monday', '__AND__', 'Tuesday', 'week', '.'], ['Day', 'One', ':', 'Ten', 'things', 'you', 'want', 'to', 'say', 'to', 'ten', 'different', 'people', 'right', 'now', '-', '1', ')', 'Eduardo', 'Surita', ':', 'your', 'a', 'freaking', '...', 'http://tumblr.com/xmciuda0t'], ['Our', 'Favorite', 'YouTube', 'Videos', 'This', 'Week', ':', 'The', 'Drama', 'Edition', 'http://dlvr.it/5RfHy'], ['I', 'just', 'took', '\"', 'After', 'getting', 'trampled', 'at', 'a', 'Justin', 'Bieber', 'concert', ',', 'yoiu', 'wake', 'up', 'and', '...\"', 'and', 'got', ':', 'Part', '6', ':)', '!', 'Try', 'it', ':', 'http://tinyurl.com/233u5lu'], ['My', 'first', 'assignment', 'was', 'a', 'medical', 'illustration', 'job', 'through', 'a', 'friend', '.', 'After', 'meeting', 'with', 'the', 'surgeon', ',', 'I', 'comple', '...'], ['@Elinor_Althani', 'ahhhh', 'reading', ',', 'still', 'i', 'did', \"n't\", 'finish', 'my', 'book', '=(', 'i', 'have', 'to', 'do', 'it', 'before', 'Feb', 'ends', 'inshallah', ':)'], ['@justinbieber', 'I', 'have', 'this', 'on', 'my', 'computer', ',', 'it', 'gives', 'me', 'days', ',', 'hours', ',', 'mintues', '__AND__', 'seconds', 'till', 'Feb', '9th', '!', '#purpleglasses', 'http://twitpic.com/3nouem'], ['i', 'see', 'some', 'ppl', 'celebrating', 'their', 'birthdays', 'like', '2-3', 'times', 'a', 'year', '...', 'WTF', '?!'], ['Class', ',', 'intelligence', ',', 'honesty', ',', 'and', 'morals', 'are', 'non', 'existent', 'these', 'days', '.'], ['still', 'fly', 'from', 'yesterday', '....', 'same', 'clothes', 'from', 'yesterday', '...', 'lmao'], ['RT', '@Luq_Combs', ':', 'Funniest', 'thing', 'I', 'heard', 'this', 'week', '.', 'Wingo', 'tellin', 'me', 'and', 'wood', '.', '\"', 'I', \"'m\", 'scared', '\"', '#pow#pow#pow@Cowboy_Wingo'], ['Gotta', 'love', 'the', 'bumper', 'stickers', 'of', 'Fayette', 'County', ':', '\"', 'Ever', 'wonder', 'if', 'there', 'is', 'life', 'after', 'death', '?', 'Touch', 'my', 'truck', 'and', \"'ll\", 'find', 'out', '.\"'], ['RT', '@4MusicGossip', ':', 'Oh', ',', 'and', 'do', \"n't\", 'forget', 'that', 'there', \"'s\", 'another', 'chance', 'to', 'see', '@pink', ':', 'My', 'Story', 'at', '7pm', 'tonight', 'http://bit.ly/hLAnXp'], ['@joejonas', '@nickjonas', '@kevinjonas', '@papajonas', '@greggarbo', '@johnlloydtaylor', 'Rock', 'to', 'SECTION', '204', 'tonight', '!!!!'], ['@christinamarks_', 'u', 'try', 'to', 'be', 'like', 'April', 'on', 'youtube'], ['@MeredithLim', 'No', '.', 'Tomorrow', '.'], ['So', 'I', \"'m\", 'currently', 'looking', 'at', 'about', '5', 'years', 'and', '3', 'months', 'if', 'I', 'can', 'get', 'out', 'of', 'there', 'within', 'the', 'next', 'month', '.'], ['RT', '@OMGwhatateen', ':', 'RT', 'if', 'you', 'are', 'happy', 'it', \"'s\", 'friday', '.', '#OMGwhatateen'], ['#blackholidays', '\"', 'National', 'forget', 'the', 'he', 'say', 'she', 'say', '...', 'what', 'do', 'God', 'say', 'day', '\"'], ['@msloaf', 'Though', 'I', 'do', 'agree', 'that', 'it', 'is', 'offensive', 'and', 'would', 'be', 'difficult', 'to', 'explain', '.', 'When', 'this', 'stuff', 'happens', 'to', 'me', 'I', 'want', 'to', 'yell', ',', '\"', 'URDUMB', '!\"'], ['RT', '@Eeenie_Meenie', ':', 'RT', 'if', 'you', 'are', 'happy', 'that', 'today', 'is', 'FRIDAY', '!', ':)'], ['People', 'who', 'tweet', 'to', 'follow', '@GuyGirlTweets', 'will', 'get', 'huge', 'shout', 'outs', 'from', 'me', ':)', 'Just', 'tell', 'me', 'when', 'you', 'do', '!'], ['Was', 'hard', 'work', 'going', 'to', 'the', 'gym', 'on', 'my', 'own', 'tonight', 'but', 'managed', 'a', '30', 'min', 'run', 'and', 'plenty', 'of', 'weights', '.', 'Catching', 'up', 'on', 'ufc', 'now'], ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], ['RT', '@DesignerDepot', ':', 'Minimalist', 'Web', 'Design', ':', 'When', 'Less', 'is', 'More', '-', 'http://ow.ly/2FwyX'], ['No', 'bike', \"t'day\", ':('], ['bruise', 'on', 'my', 'facee', '...', 'wild', 'night', '!'], ['plez', 'check', 'https://owensoto.wordpress.com/', '(', 'my', 'blog', ')', '__AND__', 'http://www.youtube.com/user/22Soto22?feature=mhum', '(', 'my', 'youtube', 'channel', ')', 'if', 'u', 'have', 'time'], ['@jaredleto', '...', 'ive', 'been', 'to', '13', '30stm', 'concerts', 'now', 'and', 'you', 'never', 'got', 'me', 'on', 'stage', 'during/before', 'kings', 'and', 'queens', ':(', '(('], ['http://bit.ly/aTTQYq', 'When', 'Pepsi', 'to', 'ring', 'usually', 'confirm', 'to', ',', 'winning', 'a', 'Nokia', '5800', '?'], ['@Jezkwon', '@kpop_stuff', 'Tweet', 'me', 'a', 'member', 'for', 'picspam', '.', 'Will', 'do', 'them', 'tomorrow', 'morning', '.', '(', ':'], ['@BexsterBexster', '@shinobi32768', 'I', \"'ve\", 'been', 'having', 'soup', 'all', 'week', ',', 'do', \"n't\", 'really', 'have', 'a', 'favourite', '.', 'It', \"'s\", 'starting', 'to', 'taste', 'pretty', 'much', 'the', 'same', '.']]\n"
     ]
    }
   ],
   "source": [
    "# function that takes in the filename for the training data\n",
    "# returns word_sequences, tag_sequences\n",
    "# word_sequences is a list in the form: [ [x_1_1, x_1_2, ...], [x_2_1, x_2_2, ...], ... [x_m_1, x_m_2, ... ] ]\n",
    "# tag_sequences is a list in the form: [ [y_1_1, y_1_2, ...], [y_2_1, y_2_2, ...], ... [y_m_1, y_m_2, ... ] ]\n",
    "def read_training_data(training_filename):\n",
    "    training_file = open(training_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    word_sequences = []\n",
    "    tag_sequences = []\n",
    "    \n",
    "    current_word_sequence = []\n",
    "    current_tag_sequence = []\n",
    "    \n",
    "    for line in training_file:\n",
    "        training_word_and_tag = line.strip().split(\" \")\n",
    "        \n",
    "        # add the current word and tag to the current word sequence and current tag sequence\n",
    "        if (len(training_word_and_tag) == 2):\n",
    "            current_word_sequence += [training_word_and_tag[0]]\n",
    "            current_tag_sequence += [training_word_and_tag[1]]\n",
    "        \n",
    "        # if the sentence ended (empty line), add the previous word sequence and tag sequence to the lists of\n",
    "        # word sequences and tag sequences respectively.\n",
    "        else:\n",
    "            word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "            tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "            \n",
    "            current_word_sequence = []\n",
    "            current_tag_sequence = []\n",
    "            \n",
    "    # account for the last word sequence\n",
    "    if (len(current_word_sequence) != 0):\n",
    "        word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "        tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "        \n",
    "    training_file.close()\n",
    "\n",
    "    return word_sequences, tag_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename for the test data\n",
    "# returns the test data as a list in the form: [ [x1_1, x1_2, ...], [x2_1, x2_2, ...] ]\n",
    "def read_test_data(test_filename):\n",
    "    test_file = open(test_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    test_word_sequences = []\n",
    "    \n",
    "    current_test_word_sequence = []\n",
    "\n",
    "    for line in test_file:\n",
    "        test_word = line.strip()\n",
    "        \n",
    "        # add current word to the current word sequence\n",
    "        if (len(test_word) != 0):\n",
    "            current_test_word_sequence += [test_word]\n",
    "            \n",
    "        # if sentence ended (len(test_word) == 0)\n",
    "        else:\n",
    "            test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "            current_test_word_sequence = []\n",
    "        \n",
    "    # account for the last word sequence\n",
    "    if (len(current_test_word_sequence) != 0):\n",
    "        test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "\n",
    "    test_file.close()\n",
    "\n",
    "    return test_word_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename of the training data and optional k value\n",
    "# returns emission_dict, tags_list\n",
    "# emission_dict[x][y] gives the value e(x|y)\n",
    "def create_emission_dict_tags_list(training_filename, k=1):\n",
    "    # emission_dict[x][y] gives the value e(x|y)\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # count_y_dict[y] gives the total number of words tagged as y\n",
    "    count_y_dict = {} \n",
    "\n",
    "    # count_x_tagged_as_y_dict[x_i][y_j] gives the number of times each observed variable x_i\n",
    "    # was tagged as state y_j in the training data\n",
    "    count_x_tagged_as_y_dict = {}\n",
    "\n",
    "    # read training data\n",
    "    word_sequences, tags_sequences = read_training_data(training_filename)\n",
    "\n",
    "    # fill up count_y_dict and count_x_tagged_as_y_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(y in count_y_dict.keys()):\n",
    "                count_y_dict[y] = 0\n",
    "\n",
    "            count_y_dict[y] += 1\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in count_x_tagged_as_y_dict.keys()): \n",
    "                count_x_tagged_as_y_dict[x] = {}\n",
    "                \n",
    "            if not(y in count_x_tagged_as_y_dict[x].keys()):\n",
    "                count_x_tagged_as_y_dict[x][y] = 0\n",
    "\n",
    "            count_x_tagged_as_y_dict[x][y] += 1\n",
    "        \n",
    "    tags_list = count_y_dict.keys()\n",
    "\n",
    "    # fill up emission_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "        \n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in emission_dict.keys()):\n",
    "                emission_dict[x] = {}\n",
    "\n",
    "            emission_dict[x][y] = count_x_tagged_as_y_dict[x][y] / (count_y_dict[y] + k)\n",
    "            \n",
    "            # add entry for \"START\" and \"END\"\n",
    "            emission_dict[x][\"START\"] = 0\n",
    "            emission_dict[x][\"END\"] = 0\n",
    "\n",
    "    # add entry for #UNK#\n",
    "    emission_dict[\"#UNK#\"] = {}\n",
    "    \n",
    "    for tag in tags_list: # iterate over all the tags used in training\n",
    "        emission_dict[\"#UNK#\"][tag] = k / (count_y_dict[tag] + k)\n",
    "\n",
    "    return emission_dict, tags_list\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns emission parameter e(x|y)\n",
    "def emission(emission_dict, tags_list, x, y, training_filename, k=1):\n",
    "    # if tag was not in training data\n",
    "    if (not(y in tags_list)):\n",
    "        print(\"This tag was not in the training data\")\n",
    "        result = 0\n",
    "\n",
    "    # else if word was not in training data\n",
    "    elif (not(x in emission_dict.keys())): # treat x as \"#UNK#\"\n",
    "        result = emission_dict[\"#UNK#\"][y] # result = k / (count_y_dict[y] + k)\n",
    "        \n",
    "    # else if word is was in training data\n",
    "    else:\n",
    "        # if x was never tagged as y before during training, the probability is 0\n",
    "        if not(y in emission_dict[x].keys()):\n",
    "            emission_dict[x][y] = 0\n",
    "\n",
    "        result = emission_dict[x][y] \n",
    "\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in a filename and a list of results in the form: [ [x1, tag1], [x2, tag2], ...]\n",
    "# writes the results to a file specified by the filename\n",
    "def write_result(result_filename, results):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for result in results:\n",
    "        # account for empty lines\n",
    "        if (len(result) == 0):\n",
    "            result_file.write(\"\\n\")\n",
    "        else:\n",
    "            result_file.write(result[0] + \" \" + result[1] + \"\\n\")\n",
    "\n",
    "    result_file.close()\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# function that takes in the filenames for the training data and test data\n",
    "# produces the tag y* = arg_max_y e(x|y) for each word in the test data\n",
    "# writes the results to a file specified by the filename\n",
    "# returns the results as a list in the form: [ [x1, y*1], [x2, y*2], ... ]\n",
    "def simple_sentiment_analysis(training_filename, test_filename, result_filename, k=1):    \n",
    "    # initialise emission_dict\n",
    "    emission_dict, tags_list = create_emission_dict_tags_list(training_filename, k)\n",
    "    \n",
    "    test_data = read_test_data(test_filename)\n",
    "    result_word_sequences = []\n",
    "    result_tag_sequences = []\n",
    "    \n",
    "    current_result_word_sequence = []\n",
    "    current_result_tag_sequence = []\n",
    "\n",
    "    for test_variable in test_data:\n",
    "        # # find the tag that gives the highest value for e(test_variable | tag)\n",
    "        if (len(test_variable) != 0):\n",
    "            predicted_tag = \"\"\n",
    "            highest_emission_value = 0\n",
    "\n",
    "            for tag in tags_list:\n",
    "                current_emission_value = emission(emission_dict, tags_list, test_variable, tag, training_filename)\n",
    "\n",
    "                if current_emission_value > highest_emission_value:\n",
    "                    highest_emission_value = current_emission_value\n",
    "                    predicted_tag = tag\n",
    "                    \n",
    "            current_result_word_sequence += []\n",
    "            results += [[test_variable, predicted_tag]]\n",
    "\n",
    "    write_result(result_filename, results)\n",
    "\n",
    "    return results, emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-27bda16fdf21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# perform the test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mtest_case_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memission_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_sentiment_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p1_test_train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"p1_test_in\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"p1_test_prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mtest_case_expected_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_case_expected_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p1_test_out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1bb8b6212182>\u001b[0m in \u001b[0;36msimple_sentiment_analysis\u001b[1;34m(training_filename, test_filename, result_filename, k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0memission_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_emission_dict_tags_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mresult_word_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0mresult_tag_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1bb8b6212182>\u001b[0m in \u001b[0;36mread_test_data\u001b[1;34m(test_filename)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mtest_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# -----------------------------------------------------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# test case\n",
    "# create training data for test case\n",
    "test_case_train_file = open(\"p1_test_train\", \"w\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word3 tag3\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.close()\n",
    "\n",
    "# create test data for test case\n",
    "test_case_test_file = open(\"p1_test_in\", \"w\")\n",
    "test_case_test_file.write(\"word1\\n\")\n",
    "test_case_test_file.write(\"word2\\n\")\n",
    "test_case_test_file.write(\"word3\\n\")\n",
    "test_case_test_file.write(\"unknown_word\")\n",
    "test_case_test_file.close()\n",
    "\n",
    "# create expected output for test case\n",
    "test_case_expected_file = open(\"p1_test_out\", \"w\")\n",
    "test_case_expected_file.write(\"word1 tag1\\n\")\n",
    "test_case_expected_file.write(\"word2 tag2\\n\")\n",
    "test_case_expected_file.write(\"word3 tag3\\n\")\n",
    "test_case_expected_file.write(\"unknown_word tag3\")\n",
    "test_case_expected_file.write(\"\\n\")\n",
    "test_case_expected_file.close()\n",
    "\n",
    "# perform the test\n",
    "test_case_prediction, emission_dict = simple_sentiment_analysis(\"p1_test_train\", \"p1_test_in\", \"p1_test_prediction\")\n",
    "test_case_expected_words, test_case_expected_labels = read_training_data(\"p1_test_out\")\n",
    "\n",
    "# show results for the test\n",
    "print(\"\\nTest case emission_dict:\")\n",
    "print(emission_dict)\n",
    "print(\"\")\n",
    "\n",
    "print(\"TESTING\")\n",
    "print(len(test_case_prediction))\n",
    "print(len(test_case_expected))\n",
    "\n",
    "for i in range(len(test_case_prediction)):\n",
    "    test_case_passed = True\n",
    "\n",
    "    if test_case_prediction[i][1] != test_case_expected[i][1]:\n",
    "        print(\"Test case failed.\")\n",
    "        print(f\"Word: {test_case_prediction[i][0]}\")\n",
    "        print(f\"Tag: {test_case_prediction[i][1]}\")\n",
    "        print(f\"Expected tag: {test_case_expected[i][1]}\\n\")\n",
    "\n",
    "        test_case_passed = False\n",
    "\n",
    "print(f\"Test case passed: {test_case_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "\n",
    "def create_transition_dict(input_list):\n",
    "    transition_dict = {}\n",
    "    start = 'START'\n",
    "    stop = 'STOP'\n",
    "    tags = set([tag for sentence in input_list for tag in sentence])\n",
    "    tags.add(start)\n",
    "    tags.add(stop)\n",
    "    for tag1 in tags:\n",
    "        for tag2 in tags:\n",
    "            count = 0\n",
    "            total = 0\n",
    "            for sentence in input_list:\n",
    "                total += len(sentence) - 1\n",
    "                for i in range(len(sentence) - 1):\n",
    "                    if sentence[i] == tag1 and sentence[i+1] == tag2:\n",
    "                        count += 1\n",
    "            if count > 0:\n",
    "                transition_dict[(tag1, tag2)] = count / total\n",
    "                \n",
    "    return transition_dict, initial_prob_dict\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns transition parameter q(yi|yi-1)\n",
    "def transition(yi_minus_1, yi):\n",
    "    # global variables\n",
    "    global transition_dict\n",
    "\n",
    "    if (yi_minus_1, yi) not in transition_dict.keys():\n",
    "        transition_dict[(yi_minus_1, yi)] = 0\n",
    "\n",
    "    result = transition_dict[(yi_minus_1, yi)]\n",
    "\n",
    "    return result\n",
    "    \n",
    "# function that takes in the filenames for the training data and test data\n",
    "# creates the table of pi values\n",
    "def viterby_first_order(training_filename, test_filename, k=1):\n",
    "    # global variables\n",
    "    global emission_dict\n",
    "    global transition_dict\n",
    "    global pi_dict\n",
    "    global tags_list\n",
    "\n",
    "    tags_list_w_start_stop = list(tags_list)\n",
    "    \n",
    "    emission_dict = create_emission_dict(training_filename, k)\n",
    "    create_transition_dict(training_filename)\n",
    "    \n",
    "    # training data as a list in the form: [ [x_1, y_1], [x_2, y_2], ...]\n",
    "    test_data = read_training_data(test_filename)\n",
    "    print(\"LENGTH TR DATA:\", len(test_data))\n",
    "    \n",
    "    \n",
    "    # initialization\n",
    "    if (\"START\" not in tags_list_w_start_stop):\n",
    "        tags_list_w_start_stop += [\"START\"]\n",
    "    \n",
    "    \n",
    "    if (\"STOP\" not in tags_list_w_start_stop):\n",
    "        tags_list_w_start_stop += [\"STOP\"]\n",
    "    \n",
    "    for x in range(0, len(test_data)+1):\n",
    "        for v in tags_list_w_start_stop:\n",
    "            if x not in pi_dict.keys():\n",
    "                pi_dict[x] = {}\n",
    "                \n",
    "            pi_dict[x][v] = 0\n",
    "        \n",
    "    pi_dict[0][\"START\"] = 1\n",
    "    \n",
    "    \n",
    "    # for each observed variable\n",
    "    for j in range(0, len(test_data)):\n",
    "        \n",
    "        x_j_plus_1 = test_data[j][0] # refers to the jth word (to calculate emission)\n",
    "        \n",
    "        # for each hidden state v\n",
    "        for v in tags_list_w_start_stop:\n",
    "            \n",
    "            # pi(j+1, v) = max over all u { pi(j,u) * transition(u, v) * emissision(x_j_plus_1, v) }\n",
    "            max_pi_val = float('-inf')\n",
    "            \n",
    "            for u in tags_list_w_start_stop:\n",
    "\n",
    "                pi = pi_dict[j][u]  \n",
    "                trans = transition(u, v)\n",
    "                emi = emission(x_j_plus_1, v, training_filename)\n",
    "                \n",
    "                if trans != 0:\n",
    "                    trans = math.log(trans)\n",
    "                if emi != 0:\n",
    "                    emi = math.log(emi)               \n",
    "                if pi > 0:\n",
    "                    pi = math.log(pi)\n",
    "                \n",
    "                current_pi_val = pi + trans + emi\n",
    "\n",
    "                # save the value that maximises\n",
    "                if (current_pi_val > max_pi_val):\n",
    "                    max_pi_val = current_pi_val\n",
    "        \n",
    "            pi_dict[j+1][v] = max_pi_val\n",
    "\n",
    "    # print(\"pii dict: \", pi_dict)\n",
    "    # print(\"trasition: \",transition(u, v))\n",
    "    # print(\"emission: \",emission(x_j_plus_1, v, training_filename))\n",
    "            \n",
    "    # final step\n",
    "    max_pi_val = float('-inf')\n",
    "    \n",
    "    # for each hidden state u\n",
    "    for u in tags_list_w_start_stop:\n",
    "        pi = pi_dict[len(test_data)][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        if pi > 0:\n",
    "            pi = math.log(pi)\n",
    "\n",
    "        current_pi_val = pi + trans\n",
    "        \n",
    "        # save the value that maximises\n",
    "        if (current_pi_val > max_pi_val):\n",
    "            max_pi_val = current_pi_val\n",
    "\n",
    "#     if max_pi_val == 0:\n",
    "#         max_pi_val = float('-inf')\n",
    "        \n",
    "    pi_dict[len(test_data)][\"STOP\"]  = max_pi_val\n",
    "\n",
    "    return pi_dict\n",
    "\n",
    "# print(transition_dict)\n",
    "# print(emission_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_viterby(test_filename, result_filename, results):\n",
    "    \n",
    "    test_data = read_test_data(test_filename)\n",
    "    \n",
    "    with open(result_filename, \"w\" ,encoding=\"utf-8\") as fp:\n",
    "        \n",
    "        for word,tag in zip(test_data, results):\n",
    "            # account for empty lines\n",
    "            if(len(word) == 0):\n",
    "                fp.write(\"\\n\")\n",
    "            else:\n",
    "                fp.write(word[0] + \" \" + tag + \"\\n\")\n",
    "    fp.close()\n",
    "\n",
    "def write_result(result_filename, results):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for result in results:\n",
    "        # account for empty lines\n",
    "        if (len(result) == 0):\n",
    "            result_file.write(\"\\n\")\n",
    "        else:\n",
    "            result_file.write(result[0] + \" \" + result[1] + \"\\n\")\n",
    "\n",
    "    result_file.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterby_backtracking(test_filename, result_filename):\n",
    "    global emission_dict\n",
    "    global transition_dict\n",
    "    global pi_dict\n",
    "    global tags_list\n",
    "    global decoding_list\n",
    "\n",
    "    tags_list_w_start_stop = list(tags_list)\n",
    "    \n",
    "    # check final layer argmax\n",
    "    argmax = float('-inf')\n",
    "    currentmax = 0\n",
    "    argmax_index = 0\n",
    "    \n",
    "    for u in tags_list_w_start_stop:\n",
    "        pi = pi_dict[len(pi_dict)-1][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        \n",
    "        if pi == 0:\n",
    "            pi = float('-inf')\n",
    "        \n",
    "        currentmax = pi + trans\n",
    "        \n",
    "        if currentmax > argmax:\n",
    "            argmax = currentmax\n",
    "            argmax_index = u\n",
    "        \n",
    "    decoding_list.append(argmax_index)\n",
    "    \n",
    "    \n",
    "    # Backtrack rest of pi_dict\n",
    "    for j in range(len(pi_dict)-2, 0, -1):\n",
    "        \n",
    "        argmax = float('-inf')\n",
    "        currentmax = 1\n",
    "        argmax_index = 0\n",
    "    \n",
    "        for u in tags_list_w_start_stop:\n",
    "    \n",
    "            pi = pi_dict[j][u]\n",
    "            trans = transition(u, decoding_list[-1])\n",
    "            \n",
    "            if trans != 0:\n",
    "                trans = math.log(trans)\n",
    "            if pi == 0:\n",
    "                pi = float('-inf')\n",
    "\n",
    "            currentmax = pi + trans\n",
    "\n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_index = u\n",
    "        \n",
    "        decoding_list.append(argmax_index)\n",
    "        \n",
    "    decoding_list = decoding_list[::-1]\n",
    "    \n",
    "    write_result_viterby(test_filename, result_filename, decoding_list)\n",
    "            \n",
    "    return decoding_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform prediction for the EN dataset\n",
    "en_results = simple_sentiment_analysis(\"EN/train\", \"EN/dev.in\", \"EN/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterby_first_order(\"EN/train\", \"EN/dev.in\")\n",
    "viterby_backtracking(\"EN/dev.in\", \"EN/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p2.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform prediction for the FR dataset\n",
    "fr_results = simple_sentiment_analysis(\"FR/train\", \"FR/dev.in\", \"FR/dev.p1.out\")\n",
    "\n",
    "# # evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viterby_first_order(\"FR/train\", \"FR/dev.in\")\n",
    "viterby_backtracking(\"FR/dev.in\", \"FR/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
