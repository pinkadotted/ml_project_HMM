{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning\n",
    "## Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Report the precision, recall and F scores of such a baseline system for each dataset:\n",
    "- EN dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.5348\n",
    "    - Entity  recall: 0.7656\n",
    "    - Entity  F: 0.6297\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.3902\n",
    "    - Sentiment  recall: 0.5586\n",
    "    - Sentiment  F: 0.4595\n",
    "- FR dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.1670\n",
    "    - Entity  recall: 0.7815\n",
    "    - Entity  F: 0.2751\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.0709\n",
    "    - Sentiment  recall: 0.3319\n",
    "    - Sentiment  F: 0.1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in the filename for the training data\n",
    "# returns the training data as a list in the form: [ [x_1, y_1], [x_2, y_2], ...]\n",
    "def read_training_data(training_filename):\n",
    "    training_file = open(training_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    word_sequences = []\n",
    "    tag_sequences = []\n",
    "    \n",
    "    current_word_sequence = []\n",
    "    current_tag_sequence = []\n",
    "    \n",
    "    for line in training_file:\n",
    "        training_word_and_tag = line.strip().split(\" \")\n",
    "        \n",
    "        # add the current word and tag to the current word sequence and current tag sequence\n",
    "        if (len(training_word_and_tag) == 2):\n",
    "            current_word_sequence += [training_word_and_tag[0]]\n",
    "            current_tag_sequence += [training_word_and_tag[1]]\n",
    "        \n",
    "        # when reaching a new sentence (empty line), add the previous word sequence and tag sequence to the lists of\n",
    "        # word sequences and tag sequences respectively. Assume the training data ends with an empty line\n",
    "        else:\n",
    "            word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "            tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "            \n",
    "            current_word_sequence = []\n",
    "            current_tag_sequence = []\n",
    "        \n",
    "    training_file.close()\n",
    "\n",
    "    return word_sequences, tag_sequences\n",
    "\n",
    "# function that takes in the filename of the training data and optional k value\n",
    "# creates/ returns the global variable \"emission_dict\". emission_dict[x][y] gives the value e(x|y)\n",
    "def create_emission_dict(training_filename, k=1):\n",
    "    # global variables\n",
    "    global emission_dict\n",
    "    global tags_list\n",
    "\n",
    "    # count_tags_dict[y] gives the total number of words tagged as y\n",
    "    count_tags_dict = {} \n",
    "\n",
    "    # count_x_tagged_as_y_dict[x_i][y_j] gives the number of times each observed variable x_i\n",
    "    # was tagged as state y_j in the training data\n",
    "    count_x_tagged_as_y_dict = {}\n",
    "\n",
    "    # read training data\n",
    "    word_sequences, tags_sequences = read_training_data(training_filename)\n",
    "\n",
    "    # fill up count_tags_dict and count_x_tagged_as_y_dict\n",
    "    for sequence_index in range(0, training_data)\n",
    "    for training_point in training_data:\n",
    "        x = training_point[0]\n",
    "        y = training_point[1]\n",
    "\n",
    "        # account for creating dictionary entry for the first time\n",
    "        if not(y in count_tags_dict.keys()):\n",
    "            count_tags_dict[y] = 0\n",
    "\n",
    "        count_tags_dict[y] += 1\n",
    "\n",
    "        # account for creating dictionary entry for the first time\n",
    "        if not(x in count_x_tagged_as_y_dict.keys()): \n",
    "            count_x_tagged_as_y_dict[x] = {}\n",
    "        if not(y in count_x_tagged_as_y_dict[x].keys()):\n",
    "            count_x_tagged_as_y_dict[x][y] = 0\n",
    "\n",
    "        count_x_tagged_as_y_dict[x][y] += 1\n",
    "        \n",
    "    tags_list = count_tags_dict.keys()\n",
    "\n",
    "    # fill up emission_dict\n",
    "    for training_point in training_data:\n",
    "        x = training_point[0]\n",
    "        y = training_point[1]\n",
    "\n",
    "        # account for creating dictionary entry for the first time\n",
    "        if not(x in emission_dict.keys()):\n",
    "            emission_dict[x] = {}\n",
    "\n",
    "        emission_dict[x][y] = count_x_tagged_as_y_dict[x][y] / (count_tags_dict[y] + k)\n",
    "\n",
    "    # add entry for #UNK#\n",
    "    emission_dict[\"#UNK#\"] = {}\n",
    "    for tag in tags_list: # iterate over all the tags used in training\n",
    "        emission_dict[\"#UNK#\"][tag] = k / (count_tags_dict[tag] + k)\n",
    "\n",
    "    return emission_dict\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns emission parameter e(x|y)\n",
    "def emission(x, y, training_filename, k=1):\n",
    "    # global variables\n",
    "    global emission_dict\n",
    "    global tags_list\n",
    "\n",
    "    # account for invalid tag\n",
    "    if not(y in tags_list):\n",
    "        # print(f\"Invalid tag: {y}\")\n",
    "        return 0\n",
    "\n",
    "    # account for case where word was not in training data\n",
    "    if not(x in emission_dict.keys()): # treat x as \"#UNK#\"\n",
    "        result = emission_dict[\"#UNK#\"][y] # result = k / (count_tags_dict[y] + k)\n",
    "    else: # if word is was in training data\n",
    "        # account for x never tagged as y before during traininng\n",
    "        if not(y in emission_dict[x].keys()):\n",
    "            emission_dict[x][y] = 0\n",
    "\n",
    "        result = emission_dict[x][y] \n",
    "\n",
    "    return result\n",
    "\n",
    "# function that takes in the filename for the test data\n",
    "# returns the test data as a list in the form: [x1, x2, ...]\n",
    "def read_test_data(test_filename):\n",
    "    test_file = open(test_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    test_data = []\n",
    "\n",
    "    for line in test_file:\n",
    "        test_point = line.strip()\n",
    "        test_data += [test_point]\n",
    "\n",
    "    test_file.close()\n",
    "\n",
    "    return test_data\n",
    "\n",
    "# function that takes in a filename and a list of results in the form: [ [x1, tag1], [x2, tag2], ...]\n",
    "# writes the results to a file specified by the filename\n",
    "def write_result(result_filename, results):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for result in results:\n",
    "        # account for empty lines\n",
    "        if (len(result) == 0):\n",
    "            result_file.write(\"\\n\")\n",
    "        else:\n",
    "            result_file.write(result[0] + \" \" + result[1] + \"\\n\")\n",
    "\n",
    "    result_file.close()\n",
    "\n",
    "# function that takes in the filenames for the training data and test data\n",
    "# produces the tag y* = arg_max_y e(x|y) for each word in the test data\n",
    "# writes the results to a file specified by the filename\n",
    "# returns the results as a list in the form: [ [x1, y*1], [x2, y*2], ... ]\n",
    "def simple_sentiment_analysis(training_filename, test_filename, result_filename, k=1):\n",
    "    global emission_dict\n",
    "    global tags_list\n",
    "\n",
    "    # reset global variables\n",
    "    emission_dict = {}\n",
    "    tags_list = []\n",
    "\n",
    "    emission_dict = {}\n",
    "    create_emission_dict(training_filename, k)\n",
    "    test_data = read_test_data(test_filename)\n",
    "    results = []\n",
    "\n",
    "    for test_variable in test_data:\n",
    "        # account for empty lines\n",
    "        if (len(test_variable) == 0):\n",
    "            results += [\"\"]\n",
    "        else: # find the tag that gives the highest value for e(test_variable | tag)\n",
    "            predicted_tag = \"\"\n",
    "            highest_emission_value = 0\n",
    "\n",
    "            for tag in tags_list:\n",
    "                current_emission_value = emission(test_variable, tag, training_filename)\n",
    "\n",
    "                if current_emission_value > highest_emission_value:\n",
    "                    highest_emission_value = current_emission_value\n",
    "                    predicted_tag = tag\n",
    "\n",
    "            results += [[test_variable, predicted_tag]]\n",
    "\n",
    "    write_result(result_filename, results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test case\n",
    "# # create training data for test case\n",
    "# test_case_train_file = open(\"p1_test_train\", \"w\")\n",
    "# test_case_train_file.write(\"word1 tag1\\n\")\n",
    "# test_case_train_file.write(\"word1 tag1\\n\")\n",
    "# test_case_train_file.write(\"word1 tag1\\n\")\n",
    "# test_case_train_file.write(\"word1 tag2\\n\")\n",
    "# test_case_train_file.write(\"\\n\")\n",
    "# test_case_train_file.write(\"word2 tag2\\n\")\n",
    "# test_case_train_file.write(\"word2 tag2\\n\")\n",
    "# test_case_train_file.write(\"word2 tag2\\n\")\n",
    "# test_case_train_file.write(\"\\n\")\n",
    "# test_case_train_file.write(\"word3 tag3\")\n",
    "# test_case_train_file.close()\n",
    "\n",
    "# # create test data for test case\n",
    "# test_case_test_file = open(\"p1_test_in\", \"w\")\n",
    "# test_case_test_file.write(\"word1\\n\")\n",
    "# test_case_test_file.write(\"word2\\n\")\n",
    "# test_case_test_file.write(\"word3\\n\")\n",
    "# test_case_test_file.write(\"unknown_word\")\n",
    "# test_case_test_file.close()\n",
    "\n",
    "# # create expected output for test case\n",
    "# test_case_expected_file = open(\"p1_test_out\", \"w\")\n",
    "# test_case_expected_file.write(\"word1 tag1\\n\")\n",
    "# test_case_expected_file.write(\"word2 tag2\\n\")\n",
    "# test_case_expected_file.write(\"word3 tag3\\n\")\n",
    "# test_case_expected_file.write(\"unknown_word tag3\")\n",
    "# test_case_expected_file.close()\n",
    "\n",
    "# # perform the test\n",
    "# test_case_prediction = simple_sentiment_analysis(\"p1_test_train\", \"p1_test_in\", \"p1_test_prediction\")\n",
    "# test_case_expected = read_training_data(\"p1_test_out\")\n",
    "\n",
    "# # show results for the test\n",
    "# print(\"\\nTest case emission_dict:\")\n",
    "# print(emission_dict)\n",
    "# print(\"\")\n",
    "\n",
    "# for i in range(len(test_case_prediction)):\n",
    "#     test_case_passed = True\n",
    "\n",
    "#     if test_case_prediction[i][1] != test_case_expected[i][1]:\n",
    "#         print(\"Test case failed.\")\n",
    "#         print(f\"Word: {test_case_prediction[i][0]}\")\n",
    "#         print(f\"Tag: {test_case_prediction[i][1]}\")\n",
    "#         print(f\"Expected tag: {test_case_expected[i][1]}\\n\")\n",
    "\n",
    "#         test_case_passed = False\n",
    "\n",
    "# print(f\"Test case passed: {test_case_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "\n",
    "def create_transition_dict(training_filename):\n",
    "    \n",
    "    global transition_dict\n",
    "    training_set = read_training_data(training_filename)\n",
    "    \n",
    "    count_yi_minus_1_yi = {}\n",
    "    count_yi_minus_1 = {}\n",
    "    \n",
    "    # Count occurrences of yi-1 and yi\n",
    "    # ===============================================\n",
    "    for i in range(len(training_set)):\n",
    "        if i == 0:\n",
    "            yi_minus_1 = 'START'\n",
    "        else:\n",
    "            yi_minus_1 = training_set[i - 1][1]\n",
    "        yi = training_set[i][1]\n",
    "        \n",
    "        # Create new entry for count(y-1) if it doesnt exist yet\n",
    "        if yi_minus_1 not in count_yi_minus_1:\n",
    "            count_yi_minus_1[yi_minus_1] = 0\n",
    "            \n",
    "        # Add +1 for the y-1 entry in the count_yi_minus_1 dict    \n",
    "        count_yi_minus_1[yi_minus_1] += 1\n",
    "        \n",
    "        # Create new entry for count(y-1,yi) if it doesnt exist yet\n",
    "        if (yi_minus_1, yi) not in count_yi_minus_1_yi:\n",
    "            count_yi_minus_1_yi[(yi_minus_1, yi)] = 0\n",
    "        \n",
    "        # Add +1 for the y-1 entry in the (count_yi_minus_1, yi) dict \n",
    "        count_yi_minus_1_yi[(yi_minus_1, yi)] += 1\n",
    "\n",
    "        \n",
    "    # Calculate transition and put it inside transition_dict\n",
    "    # ===============================================\n",
    "    for (yi_minus_1, yi), count in count_yi_minus_1_yi.items():\n",
    "        transition_dict[(yi_minus_1, yi)] = count / count_yi_minus_1[yi_minus_1]\n",
    "\n",
    "        \n",
    "    # Calculate q(STOP|yn) and q(y1|START)\n",
    "    # ===============================================\n",
    "    y_n = training_set[-1][1]\n",
    "    \n",
    "    # Add entry in dicts for 'STOP'\n",
    "    transition_dict[(y_n, 'STOP')] = 0\n",
    "    count_yi_minus_1['STOP'] = 0\n",
    "    \n",
    "    # Basically 1 / Count(y_n)\n",
    "    # +1 for count_yi_minus_1[y_n] + 1 as it does not contain the last y_n\n",
    "    transition_dict[(y_n, 'STOP')] = 1 / (count_yi_minus_1[y_n] + 1)\n",
    "    \n",
    "    # Technically always 1 since there's only 1 start and one combi of (y0, START)\n",
    "    transition_dict[('START', training_set[0][1])] = 1 / (count_yi_minus_1['START'])\n",
    "\n",
    "    return transition_dict\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns transition parameter q(yi|yi-1)\n",
    "def transition(yi_minus_1, yi):\n",
    "    # global variables\n",
    "    global transition_dict\n",
    "\n",
    "    if (yi_minus_1, yi) not in transition_dict.keys():\n",
    "        transition_dict[(yi_minus_1, yi)] = 0\n",
    "\n",
    "    result = transition_dict[(yi_minus_1, yi)]\n",
    "\n",
    "    return result\n",
    "    \n",
    "# function that takes in the filenames for the training data and test data\n",
    "# creates the table of pi values\n",
    "def viterby_first_order(training_filename, test_filename, k=1):\n",
    "    # global variables\n",
    "    global emission_dict\n",
    "    global transition_dict\n",
    "    global pi_dict\n",
    "    global tags_list\n",
    "\n",
    "    tags_list_w_start_stop = list(tags_list)\n",
    "    \n",
    "    emission_dict = create_emission_dict(training_filename, k)\n",
    "    create_transition_dict(training_filename)\n",
    "    \n",
    "    # training data as a list in the form: [ [x_1, y_1], [x_2, y_2], ...]\n",
    "    test_data = read_training_data(test_filename)\n",
    "    print(\"LENGTH TR DATA:\", len(test_data))\n",
    "    \n",
    "    \n",
    "    # initialization\n",
    "    if (\"START\" not in tags_list_w_start_stop):\n",
    "        tags_list_w_start_stop += [\"START\"]\n",
    "    \n",
    "    \n",
    "    if (\"STOP\" not in tags_list_w_start_stop):\n",
    "        tags_list_w_start_stop += [\"STOP\"]\n",
    "    \n",
    "    for x in range(0, len(test_data)+1):\n",
    "        for v in tags_list_w_start_stop:\n",
    "            if x not in pi_dict.keys():\n",
    "                pi_dict[x] = {}\n",
    "                \n",
    "            pi_dict[x][v] = 0\n",
    "        \n",
    "    pi_dict[0][\"START\"] = 1\n",
    "    \n",
    "    \n",
    "    # for each observed variable\n",
    "    for j in range(0, len(test_data)):\n",
    "        \n",
    "        x_j_plus_1 = test_data[j][0] # refers to the jth word (to calculate emission)\n",
    "        \n",
    "        # for each hidden state v\n",
    "        for v in tags_list_w_start_stop:\n",
    "            \n",
    "            # pi(j+1, v) = max over all u { pi(j,u) * transition(u, v) * emissision(x_j_plus_1, v) }\n",
    "            max_pi_val = float('-inf')\n",
    "            \n",
    "            for u in tags_list_w_start_stop:\n",
    "\n",
    "                pi = pi_dict[j][u]  \n",
    "                trans = transition(u, v)\n",
    "                emi = emission(x_j_plus_1, v, training_filename)\n",
    "                \n",
    "                if trans != 0:\n",
    "                    trans = math.log(trans)\n",
    "                if emi != 0:\n",
    "                    emi = math.log(emi)               \n",
    "                if pi > 0:\n",
    "                    pi = math.log(pi)\n",
    "                \n",
    "                current_pi_val = pi + trans + emi\n",
    "\n",
    "                # save the value that maximises\n",
    "                if (current_pi_val > max_pi_val):\n",
    "                    max_pi_val = current_pi_val\n",
    "        \n",
    "            pi_dict[j+1][v] = max_pi_val\n",
    "\n",
    "    # print(\"pii dict: \", pi_dict)\n",
    "    # print(\"trasition: \",transition(u, v))\n",
    "    # print(\"emission: \",emission(x_j_plus_1, v, training_filename))\n",
    "            \n",
    "    # final step\n",
    "    max_pi_val = float('-inf')\n",
    "    \n",
    "    # for each hidden state u\n",
    "    for u in tags_list_w_start_stop:\n",
    "        pi = pi_dict[len(test_data)][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        if pi > 0:\n",
    "            pi = math.log(pi)\n",
    "\n",
    "        current_pi_val = pi + trans\n",
    "        \n",
    "        # save the value that maximises\n",
    "        if (current_pi_val > max_pi_val):\n",
    "            max_pi_val = current_pi_val\n",
    "\n",
    "#     if max_pi_val == 0:\n",
    "#         max_pi_val = float('-inf')\n",
    "        \n",
    "    pi_dict[len(test_data)][\"STOP\"]  = max_pi_val\n",
    "\n",
    "    return pi_dict\n",
    "\n",
    "# print(transition_dict)\n",
    "# print(emission_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_viterby(test_filename, result_filename, results):\n",
    "    \n",
    "    test_data = read_test_data(test_filename)\n",
    "    \n",
    "    with open(result_filename, \"w\" ,encoding=\"utf-8\") as fp:\n",
    "        \n",
    "        for word,tag in zip(test_data, results):\n",
    "            # account for empty lines\n",
    "            if(len(word) == 0):\n",
    "                fp.write(\"\\n\")\n",
    "            else:\n",
    "                fp.write(word[0] + \" \" + tag + \"\\n\")\n",
    "    fp.close()\n",
    "\n",
    "def write_result(result_filename, results):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for result in results:\n",
    "        # account for empty lines\n",
    "        if (len(result) == 0):\n",
    "            result_file.write(\"\\n\")\n",
    "        else:\n",
    "            result_file.write(result[0] + \" \" + result[1] + \"\\n\")\n",
    "\n",
    "    result_file.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterby_backtracking(test_filename, result_filename):\n",
    "    global emission_dict\n",
    "    global transition_dict\n",
    "    global pi_dict\n",
    "    global tags_list\n",
    "    global decoding_list\n",
    "\n",
    "    tags_list_w_start_stop = list(tags_list)\n",
    "    \n",
    "    # check final layer argmax\n",
    "    argmax = float('-inf')\n",
    "    currentmax = 0\n",
    "    argmax_index = 0\n",
    "    \n",
    "    for u in tags_list_w_start_stop:\n",
    "        pi = pi_dict[len(pi_dict)-1][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        \n",
    "        if pi == 0:\n",
    "            pi = float('-inf')\n",
    "        \n",
    "        currentmax = pi + trans\n",
    "        \n",
    "        if currentmax > argmax:\n",
    "            argmax = currentmax\n",
    "            argmax_index = u\n",
    "        \n",
    "    decoding_list.append(argmax_index)\n",
    "    \n",
    "    \n",
    "    # Backtrack rest of pi_dict\n",
    "    for j in range(len(pi_dict)-2, 0, -1):\n",
    "        \n",
    "        argmax = float('-inf')\n",
    "        currentmax = 1\n",
    "        argmax_index = 0\n",
    "    \n",
    "        for u in tags_list_w_start_stop:\n",
    "    \n",
    "            pi = pi_dict[j][u]\n",
    "            trans = transition(u, decoding_list[-1])\n",
    "            \n",
    "            if trans != 0:\n",
    "                trans = math.log(trans)\n",
    "            if pi == 0:\n",
    "                pi = float('-inf')\n",
    "\n",
    "            currentmax = pi + trans\n",
    "\n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_index = u\n",
    "        \n",
    "        decoding_list.append(argmax_index)\n",
    "        \n",
    "    decoding_list = decoding_list[::-1]\n",
    "    \n",
    "    write_result_viterby(test_filename, result_filename, decoding_list)\n",
    "            \n",
    "    return decoding_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform prediction for the EN dataset\n",
    "en_results = simple_sentiment_analysis(\"EN/train\", \"EN/dev.in\", \"EN/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterby_first_order(\"EN/train\", \"EN/dev.in\")\n",
    "viterby_backtracking(\"EN/dev.in\", \"EN/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p2.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform prediction for the FR dataset\n",
    "fr_results = simple_sentiment_analysis(\"FR/train\", \"FR/dev.in\", \"FR/dev.p1.out\")\n",
    "\n",
    "# # evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viterby_first_order(\"FR/train\", \"FR/dev.in\")\n",
    "viterby_backtracking(\"FR/dev.in\", \"FR/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
