{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50.007 Machine Learning\n",
    "## Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Report the precision, recall and F scores of such a baseline system for each dataset:\n",
    "- EN dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.5348\n",
    "    - Entity  recall: 0.7656\n",
    "    - Entity  F: 0.6297\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.3902\n",
    "    - Sentiment  recall: 0.5586\n",
    "    - Sentiment  F: 0.4595\n",
    "- FR dataset\n",
    "  - Entity scores:\n",
    "    - Entity  precision: 0.1670\n",
    "    - Entity  recall: 0.7815\n",
    "    - Entity  F: 0.2751\n",
    "  - Sentiment scores:\n",
    "    - Sentiment  precision: 0.0709\n",
    "    - Sentiment  recall: 0.3319\n",
    "    - Sentiment  F: 0.1169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes in the filename for the training data\n",
    "# returns word_sequences, tag_sequences\n",
    "# word_sequences is a list in the form: [ [x_1_1, x_1_2, ...], [x_2_1, x_2_2, ...], ... [x_m_1, x_m_2, ... ] ]\n",
    "# tag_sequences is a list in the form: [ [y_1_1, y_1_2, ...], [y_2_1, y_2_2, ...], ... [y_m_1, y_m_2, ... ] ]\n",
    "def read_training_data(training_filename):\n",
    "    training_file = open(training_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    word_sequences = []\n",
    "    tag_sequences = []\n",
    "    \n",
    "    current_word_sequence = []\n",
    "    current_tag_sequence = []\n",
    "    \n",
    "    for line in training_file:\n",
    "        training_word_and_tag = line.strip().split(\" \")\n",
    "        \n",
    "        # add the current word and tag to the current word sequence and current tag sequence\n",
    "        if (len(training_word_and_tag) == 2):\n",
    "            current_word_sequence += [training_word_and_tag[0]]\n",
    "            current_tag_sequence += [training_word_and_tag[1]]\n",
    "        \n",
    "        # if the sentence ended (empty line), add the previous word sequence and tag sequence to the lists of\n",
    "        # word sequences and tag sequences respectively.\n",
    "        else:\n",
    "            word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "            tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "            \n",
    "            current_word_sequence = []\n",
    "            current_tag_sequence = []\n",
    "            \n",
    "    # account for the last word sequence\n",
    "    if (len(current_word_sequence) != 0):\n",
    "        word_sequences += [copy.deepcopy(current_word_sequence)]\n",
    "        tag_sequences += [copy.deepcopy(current_tag_sequence)]\n",
    "        \n",
    "    training_file.close()\n",
    "\n",
    "    return word_sequences, tag_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename for the test data\n",
    "# returns the test data as a list in the form: [ [x1_1, x1_2, ...], [x2_1, x2_2, ...] ]\n",
    "def read_test_data(test_filename):\n",
    "    test_file = open(test_filename, \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    test_word_sequences = []\n",
    "    \n",
    "    current_test_word_sequence = []\n",
    "\n",
    "    for line in test_file:\n",
    "        test_word = line.strip()\n",
    "        \n",
    "        # add current word to the current word sequence\n",
    "        if (len(test_word) != 0):\n",
    "            current_test_word_sequence += [test_word]\n",
    "            \n",
    "        # if sentence ended (len(test_word) == 0)\n",
    "        else:\n",
    "            test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "            current_test_word_sequence = []\n",
    "        \n",
    "    # account for the last word sequence\n",
    "    if (len(current_test_word_sequence) != 0):\n",
    "        test_word_sequences += [copy.deepcopy(current_test_word_sequence)]\n",
    "\n",
    "    test_file.close()\n",
    "\n",
    "    return test_word_sequences\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in the filename of the training data and optional k value\n",
    "# returns emission_dict, tags_list\n",
    "# emission_dict[x][y] gives the value e(x|y)\n",
    "def create_emission_dict_tags_list(training_filename, k=1):\n",
    "    # emission_dict[x][y] gives the value e(x|y)\n",
    "    emission_dict = {}\n",
    "    \n",
    "    # count_y_dict[y] gives the total number of words tagged as y\n",
    "    count_y_dict = {} \n",
    "\n",
    "    # count_x_tagged_as_y_dict[x_i][y_j] gives the number of times each observed variable x_i\n",
    "    # was tagged as state y_j in the training data\n",
    "    count_x_tagged_as_y_dict = {}\n",
    "\n",
    "    # read training data\n",
    "    word_sequences, tags_sequences = read_training_data(training_filename)\n",
    "\n",
    "    # fill up count_y_dict and count_x_tagged_as_y_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(y in count_y_dict.keys()):\n",
    "                count_y_dict[y] = 0\n",
    "\n",
    "            count_y_dict[y] += 1\n",
    "\n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in count_x_tagged_as_y_dict.keys()): \n",
    "                count_x_tagged_as_y_dict[x] = {}\n",
    "                \n",
    "            if not(y in count_x_tagged_as_y_dict[x].keys()):\n",
    "                count_x_tagged_as_y_dict[x][y] = 0\n",
    "\n",
    "            count_x_tagged_as_y_dict[x][y] += 1\n",
    "        \n",
    "    tags_list = count_y_dict.keys()\n",
    "\n",
    "    # fill up emission_dict\n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            x = word_sequences[sequence_index][word_index]\n",
    "            y = tags_sequences[sequence_index][word_index]\n",
    "        \n",
    "            # account for creating dictionary entry for the first time\n",
    "            if not(x in emission_dict.keys()):\n",
    "                emission_dict[x] = {}\n",
    "\n",
    "            emission_dict[x][y] = count_x_tagged_as_y_dict[x][y] / (count_y_dict[y] + k)\n",
    "            \n",
    "            # add entry for \"START\" and \"END\"\n",
    "            emission_dict[x][\"START\"] = 0\n",
    "            emission_dict[x][\"END\"] = 0\n",
    "\n",
    "    # add entry for #UNK#\n",
    "    emission_dict[\"#UNK#\"] = {}\n",
    "    \n",
    "    for tag in tags_list: # iterate over all the tags used in training\n",
    "        emission_dict[\"#UNK#\"][tag] = k / (count_y_dict[tag] + k)\n",
    "\n",
    "    return emission_dict, tags_list\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns emission parameter e(x|y)\n",
    "def emission(emission_dict, tags_list, x, y, k=1):\n",
    "    # if tag was not in training data\n",
    "    if (not(y in tags_list)):\n",
    "        print(\"This tag was not in the training data\")\n",
    "        result = 0\n",
    "\n",
    "    # else if word was not in training data\n",
    "    elif (not(x in emission_dict.keys())): # treat x as \"#UNK#\"\n",
    "        result = emission_dict[\"#UNK#\"][y] # result = k / (count_y_dict[y] + k)\n",
    "        \n",
    "    # else if word is was in training data\n",
    "    else:\n",
    "        # if x was never tagged as y before during training, the probability is 0\n",
    "        if not(y in emission_dict[x].keys()):\n",
    "            emission_dict[x][y] = 0\n",
    "\n",
    "        result = emission_dict[x][y] \n",
    "\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in a filename and a list of results in the form: [ [x1, tag1], [x2, tag2], ...]\n",
    "# writes the results to a file specified by the filename\n",
    "def write_result(result_filename, word_sequences, tag_sequences):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for sequence_index in range(0, len(word_sequences)):\n",
    "        for word_index in range(0, len(word_sequences[sequence_index])):\n",
    "            result_file.write(word_sequences[sequence_index][word_index] + \" \" + tag_sequences[sequence_index][word_index] + \"\\n\")\n",
    "            \n",
    "        result_file.write(\"\\n\")\n",
    "\n",
    "    result_file.close()\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# function that takes in the filenames for the training data and test data\n",
    "# produces the tag y* = arg_max_y e(x|y) for each word in the test data\n",
    "# writes the results to a file specified by the filename\n",
    "# returns the results as a list in the form: [ [x1, y*1], [x2, y*2], ... ]\n",
    "def simple_sentiment_analysis(training_filename, test_filename, result_filename, k=1):    \n",
    "    # initialise emission_dict\n",
    "    emission_dict, tags_list = create_emission_dict_tags_list(training_filename, k)\n",
    "    \n",
    "    test_word_sequences = read_test_data(test_filename)\n",
    "    \n",
    "    prediction_tag_sequences = []\n",
    "\n",
    "    for test_word_sequence in test_word_sequences:\n",
    "        current_prediction_tag_sequence = []\n",
    "        \n",
    "        for test_word in test_word_sequence:\n",
    "            # find the tag that gives the highest value for e(test_variable | tag)\n",
    "            predicted_tag = \"\"\n",
    "            highest_emission_value = 0\n",
    "\n",
    "            for tag in tags_list:\n",
    "                current_emission_value = emission(emission_dict, tags_list, test_word, tag)\n",
    "\n",
    "                if current_emission_value > highest_emission_value:\n",
    "                    highest_emission_value = current_emission_value\n",
    "                    predicted_tag = tag\n",
    "\n",
    "            current_prediction_tag_sequence += [predicted_tag]\n",
    "            \n",
    "        # at the end of the sentence, add the current prediction tag sequence to the lise prediction_tag_sequences\n",
    "        prediction_tag_sequences += [copy.deepcopy(current_prediction_tag_sequence)]\n",
    "        current_prediction_tag_sequence = []\n",
    "\n",
    "    write_result(result_filename, test_word_sequences, prediction_tag_sequences)\n",
    "\n",
    "    return test_word_sequences, prediction_tag_sequences, emission_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 802\n",
      "#Entity in prediction: 1148\n",
      "\n",
      "#Correct Entity : 614\n",
      "Entity  precision: 0.5348\n",
      "Entity  recall: 0.7656\n",
      "Entity  F: 0.6297\n",
      "\n",
      "#Correct Sentiment : 448\n",
      "Sentiment  precision: 0.3902\n",
      "Sentiment  recall: 0.5586\n",
      "Sentiment  F: 0.4595\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the EN dataset\n",
    "en_results = simple_sentiment_analysis(\"EN/train\", \"EN/dev.in\", \"EN/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Entity in gold data: 238\n",
      "#Entity in prediction: 1114\n",
      "\n",
      "#Correct Entity : 186\n",
      "Entity  precision: 0.1670\n",
      "Entity  recall: 0.7815\n",
      "Entity  F: 0.2751\n",
      "\n",
      "#Correct Sentiment : 79\n",
      "Sentiment  precision: 0.0709\n",
      "Sentiment  recall: 0.3319\n",
      "Sentiment  F: 0.1169\n"
     ]
    }
   ],
   "source": [
    "# perform prediction for the FR dataset\n",
    "fr_results = simple_sentiment_analysis(\"FR/train\", \"FR/dev.in\", \"FR/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case emission_dict:\n",
      "{'word1': {'tag1': 0.75, 'START': 0, 'END': 0, 'tag2': 0.2, 'tag3': 0}, 'word2': {'tag2': 0.6, 'START': 0, 'END': 0, 'tag1': 0, 'tag3': 0}, 'word3': {'tag3': 0.5, 'START': 0, 'END': 0, 'tag1': 0, 'tag2': 0}, '#UNK#': {'tag1': 0.25, 'tag2': 0.2, 'tag3': 0.5}}\n",
      "\n",
      "Test case passed: True\n"
     ]
    }
   ],
   "source": [
    "# part 1 test case\n",
    "# create training data for test case\n",
    "test_case_train_file = open(\"p1_test_train\", \"w\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag1\\n\")\n",
    "test_case_train_file.write(\"word1 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"word2 tag2\\n\")\n",
    "test_case_train_file.write(\"\\n\")\n",
    "test_case_train_file.write(\"word3 tag3\")\n",
    "test_case_train_file.close()\n",
    "\n",
    "# create test data for test case\n",
    "test_case_test_file = open(\"p1_test_in\", \"w\")\n",
    "test_case_test_file.write(\"word1\\n\")\n",
    "test_case_test_file.write(\"word2\\n\")\n",
    "test_case_test_file.write(\"word3\\n\")\n",
    "test_case_test_file.write(\"unknown_word\")\n",
    "test_case_test_file.close()\n",
    "\n",
    "# create expected output for test case\n",
    "test_case_expected_file = open(\"p1_test_out\", \"w\")\n",
    "test_case_expected_file.write(\"word1 tag1\\n\")\n",
    "test_case_expected_file.write(\"word2 tag2\\n\")\n",
    "test_case_expected_file.write(\"word3 tag3\\n\")\n",
    "test_case_expected_file.write(\"unknown_word tag3\")\n",
    "test_case_expected_file.close()\n",
    "\n",
    "# perform the test\n",
    "test_word_sequences, prediction_tag_sequences, emission_dict = simple_sentiment_analysis(\"p1_test_train\", \"p1_test_in\", \"p1_test_prediction\")\n",
    "test_word_sequences, expected_tag_sequences= read_training_data(\"p1_test_out\")\n",
    "\n",
    "# show results for the test\n",
    "print(\"\\nTest case emission_dict:\")\n",
    "print(emission_dict)\n",
    "print(\"\")\n",
    "\n",
    "test_case_passed = True\n",
    "\n",
    "for sequence_index in range(0, len(test_word_sequences)):\n",
    "    for tag_index in range(0, len(test_word_sequences[sequence_index])):\n",
    "        if prediction_tag_sequences[sequence_index][tag_index] != expected_tag_sequences[sequence_index][tag_index]:\n",
    "            test_case_passed = False\n",
    "            \n",
    "            print(\"Test case failed.\")\n",
    "            print(f\"Word: {test_case_prediction[i][0]}\")\n",
    "            print(f\"Tag: {test_case_prediction[i][1]}\")\n",
    "            print(f\"Expected tag: {test_case_expected[i][1]}\\n\")\n",
    "\n",
    "print(f\"Test case passed: {test_case_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'O': 0,\n",
       "  'B-INTJ': 0,\n",
       "  'B-PP': 0,\n",
       "  'B-NP': 0,\n",
       "  'I-NP': 0,\n",
       "  'B-VP': 0,\n",
       "  'B-PRT': 0,\n",
       "  'I-VP': 0,\n",
       "  'B-ADJP': 0,\n",
       "  'B-SBAR': 0,\n",
       "  'B-ADVP': 0,\n",
       "  'I-INTJ': 0,\n",
       "  'B-CONJP': 0,\n",
       "  'I-CONJP': 0,\n",
       "  'I-ADVP': 0,\n",
       "  'I-ADJP': 0,\n",
       "  'I-SBAR': 0,\n",
       "  'I-PP': 0,\n",
       "  'START': 1},\n",
       " 1: {'O': -7.692569648067906,\n",
       "  'B-INTJ': -5.552959584921617,\n",
       "  'B-PP': -6.57507584059962,\n",
       "  'B-NP': -7.926241523170962,\n",
       "  'I-NP': -7.552237287560802,\n",
       "  'B-VP': -7.191429330036379,\n",
       "  'B-PRT': -4.454347296253507,\n",
       "  'I-VP': -6.410174881966167,\n",
       "  'B-ADJP': -5.075173815233827,\n",
       "  'B-SBAR': -4.276666119016055,\n",
       "  'B-ADVP': -5.958424693029782,\n",
       "  'I-INTJ': -4.48863636973214,\n",
       "  'B-CONJP': -1.0986122886681098,\n",
       "  'I-CONJP': -1.0986122886681098,\n",
       "  'I-ADVP': -3.951243718581427,\n",
       "  'I-ADJP': -4.127134385045092,\n",
       "  'I-SBAR': -1.6094379124341003,\n",
       "  'I-PP': -1.9459101490553135},\n",
       " 2: {'O': -8.791181936736015,\n",
       "  'B-INTJ': -6.651571873589727,\n",
       "  'B-PP': -7.67368812926773,\n",
       "  'B-NP': -9.02485381183907,\n",
       "  'I-NP': -8.65084957622891,\n",
       "  'B-VP': -8.290041618704489,\n",
       "  'B-PRT': -5.552959584921617,\n",
       "  'I-VP': -7.508787170634277,\n",
       "  'B-ADJP': -6.173786103901937,\n",
       "  'B-SBAR': -5.375278407684165,\n",
       "  'B-ADVP': -7.057036981697892,\n",
       "  'I-INTJ': -5.58724865840025,\n",
       "  'B-CONJP': -2.1972245773362196,\n",
       "  'I-CONJP': -2.1972245773362196,\n",
       "  'I-ADVP': -5.049856007249537,\n",
       "  'I-ADJP': -5.225746673713202,\n",
       "  'I-SBAR': -2.70805020110221,\n",
       "  'I-PP': -3.0445224377234235},\n",
       " 3: {'O': -9.889794225404126,\n",
       "  'B-INTJ': -7.7501841622578365,\n",
       "  'B-PP': -8.77230041793584,\n",
       "  'B-NP': -10.123466100507182,\n",
       "  'I-NP': -9.749461864897022,\n",
       "  'B-VP': -9.388653907372598,\n",
       "  'B-PRT': -6.651571873589727,\n",
       "  'I-VP': -8.607399459302387,\n",
       "  'B-ADJP': -7.272398392570047,\n",
       "  'B-SBAR': -6.473890696352274,\n",
       "  'B-ADVP': -8.155649270366002,\n",
       "  'I-INTJ': -6.68586094706836,\n",
       "  'B-CONJP': -3.295836866004329,\n",
       "  'I-CONJP': -3.295836866004329,\n",
       "  'I-ADVP': -6.148468295917647,\n",
       "  'I-ADJP': -6.324358962381311,\n",
       "  'I-SBAR': -3.8066624897703196,\n",
       "  'I-PP': -4.1431347263915335},\n",
       " 4: {'O': -5.239013528163982,\n",
       "  'B-INTJ': -8.848796450925946,\n",
       "  'B-PP': -3.295836866004329,\n",
       "  'B-NP': -3.295836866004329,\n",
       "  'I-NP': -8.209016823949872,\n",
       "  'B-VP': -3.295836866004329,\n",
       "  'B-PRT': -3.295836866004329,\n",
       "  'I-VP': -3.295836866004329,\n",
       "  'B-ADJP': -3.295836866004329,\n",
       "  'B-SBAR': -3.295836866004329,\n",
       "  'B-ADVP': -3.295836866004329,\n",
       "  'I-INTJ': -6.68586094706836,\n",
       "  'B-CONJP': -3.295836866004329,\n",
       "  'I-CONJP': -3.295836866004329,\n",
       "  'I-ADVP': -3.295836866004329,\n",
       "  'I-ADJP': -3.295836866004329,\n",
       "  'I-SBAR': -3.295836866004329,\n",
       "  'I-PP': -3.295836866004329},\n",
       " 5: {'O': -3.295836866004329,\n",
       "  'B-INTJ': -3.295836866004329,\n",
       "  'B-PP': -3.295836866004329,\n",
       "  'B-NP': -3.295836866004329,\n",
       "  'I-NP': -10.84807415356513,\n",
       "  'B-VP': -3.295836866004329,\n",
       "  'B-PRT': -3.295836866004329,\n",
       "  'I-VP': -9.706011747970496,\n",
       "  'B-ADJP': -3.295836866004329,\n",
       "  'B-SBAR': -3.295836866004329,\n",
       "  'B-ADVP': -3.295836866004329,\n",
       "  'I-INTJ': -3.295836866004329,\n",
       "  'B-CONJP': -3.295836866004329,\n",
       "  'I-CONJP': -3.295836866004329,\n",
       "  'I-ADVP': -3.295836866004329,\n",
       "  'I-ADJP': -3.295836866004329,\n",
       "  'I-SBAR': -3.295836866004329,\n",
       "  'I-PP': -3.295836866004329},\n",
       " 6: {'O': -10.988406514072235,\n",
       "  'B-INTJ': -8.848796450925946,\n",
       "  'B-PP': -9.87091270660395,\n",
       "  'B-NP': -11.22207838917529,\n",
       "  'I-NP': -10.84807415356513,\n",
       "  'B-VP': -10.487266196040707,\n",
       "  'B-PRT': -7.7501841622578365,\n",
       "  'I-VP': -9.706011747970496,\n",
       "  'B-ADJP': -8.371010681238156,\n",
       "  'B-SBAR': -7.572502985020384,\n",
       "  'B-ADVP': -9.254261559034111,\n",
       "  'I-INTJ': -7.784473235736469,\n",
       "  'B-CONJP': -4.394449154672439,\n",
       "  'I-CONJP': -4.394449154672439,\n",
       "  'I-ADVP': -7.247080584585756,\n",
       "  'I-ADJP': -7.422971251049421,\n",
       "  'I-SBAR': -4.90527477843843,\n",
       "  'I-PP': -5.241747015059643},\n",
       " 7: {'O': -4.394449154672439,\n",
       "  'B-INTJ': -9.947408739594056,\n",
       "  'B-PP': -4.394449154672439,\n",
       "  'B-NP': -4.394449154672439,\n",
       "  'I-NP': -4.394449154672439,\n",
       "  'B-VP': -11.585878484708818,\n",
       "  'B-PRT': -4.394449154672439,\n",
       "  'I-VP': -4.394449154672439,\n",
       "  'B-ADJP': -4.394449154672439,\n",
       "  'B-SBAR': -4.394449154672439,\n",
       "  'B-ADVP': -4.394449154672439,\n",
       "  'I-INTJ': -4.394449154672439,\n",
       "  'B-CONJP': -4.394449154672439,\n",
       "  'I-CONJP': -4.394449154672439,\n",
       "  'I-ADVP': -4.394449154672439,\n",
       "  'I-ADJP': -4.394449154672439,\n",
       "  'I-SBAR': -4.394449154672439,\n",
       "  'I-PP': -4.394449154672439},\n",
       " 8: {'O': -4.394449154672439,\n",
       "  'B-INTJ': -4.394449154672439,\n",
       "  'B-PP': -4.394449154672439,\n",
       "  'B-NP': -4.394449154672439,\n",
       "  'I-NP': -4.394449154672439,\n",
       "  'B-VP': -10.892731304148873,\n",
       "  'B-PRT': -4.394449154672439,\n",
       "  'I-VP': -4.394449154672439,\n",
       "  'B-ADJP': -4.394449154672439,\n",
       "  'B-SBAR': -4.394449154672439,\n",
       "  'B-ADVP': -4.394449154672439,\n",
       "  'I-INTJ': -4.394449154672439,\n",
       "  'B-CONJP': -4.394449154672439,\n",
       "  'I-CONJP': -4.394449154672439,\n",
       "  'I-ADVP': -4.394449154672439,\n",
       "  'I-ADJP': -4.394449154672439,\n",
       "  'I-SBAR': -4.394449154672439,\n",
       "  'I-PP': -4.394449154672439},\n",
       " 9: {'O': -10.700724441620455,\n",
       "  'B-INTJ': -4.394449154672439,\n",
       "  'B-PP': -10.96952499527206,\n",
       "  'B-NP': -4.394449154672439,\n",
       "  'I-NP': -11.94668644223324,\n",
       "  'B-VP': -4.394449154672439,\n",
       "  'B-PRT': -4.394449154672439,\n",
       "  'I-VP': -4.394449154672439,\n",
       "  'B-ADJP': -4.394449154672439,\n",
       "  'B-SBAR': -4.394449154672439,\n",
       "  'B-ADVP': -4.394449154672439,\n",
       "  'I-INTJ': -4.394449154672439,\n",
       "  'B-CONJP': -4.394449154672439,\n",
       "  'I-CONJP': -4.394449154672439,\n",
       "  'I-ADVP': -4.394449154672439,\n",
       "  'I-ADJP': -8.52158353971753,\n",
       "  'I-SBAR': -4.394449154672439,\n",
       "  'I-PP': -4.394449154672439},\n",
       " 10: {'O': -12.087018802740346,\n",
       "  'B-INTJ': -9.947408739594056,\n",
       "  'B-PP': -10.96952499527206,\n",
       "  'B-NP': -12.320690677843402,\n",
       "  'I-NP': -11.94668644223324,\n",
       "  'B-VP': -11.585878484708818,\n",
       "  'B-PRT': -8.848796450925946,\n",
       "  'I-VP': -10.804624036638607,\n",
       "  'B-ADJP': -9.469622969906265,\n",
       "  'B-SBAR': -8.671115273688494,\n",
       "  'B-ADVP': -10.35287384770222,\n",
       "  'I-INTJ': -8.883085524404578,\n",
       "  'B-CONJP': -5.493061443340549,\n",
       "  'I-CONJP': -5.493061443340549,\n",
       "  'I-ADVP': -8.345692873253867,\n",
       "  'I-ADJP': -8.52158353971753,\n",
       "  'I-SBAR': -6.00388706710654,\n",
       "  'I-PP': -6.340359303727753},\n",
       " 11: {'O': -5.493061443340549,\n",
       "  'B-INTJ': -5.493061443340549,\n",
       "  'B-PP': -5.493061443340549,\n",
       "  'B-NP': -5.493061443340549,\n",
       "  'I-NP': -13.04529873090135,\n",
       "  'B-VP': -5.493061443340549,\n",
       "  'B-PRT': -5.493061443340549,\n",
       "  'I-VP': -5.493061443340549,\n",
       "  'B-ADJP': -5.493061443340549,\n",
       "  'B-SBAR': -5.493061443340549,\n",
       "  'B-ADVP': -11.451486136370331,\n",
       "  'I-INTJ': -5.493061443340549,\n",
       "  'B-CONJP': -5.493061443340549,\n",
       "  'I-CONJP': -5.493061443340549,\n",
       "  'I-ADVP': -5.493061443340549,\n",
       "  'I-ADJP': -5.493061443340549,\n",
       "  'I-SBAR': -5.493061443340549,\n",
       "  'I-PP': -5.493061443340549},\n",
       " 12: {'O': -13.185631091408455,\n",
       "  'B-INTJ': -11.046021028262167,\n",
       "  'B-PP': -12.06813728394017,\n",
       "  'B-NP': -13.419302966511511,\n",
       "  'I-NP': -13.04529873090135,\n",
       "  'B-VP': -12.684490773376929,\n",
       "  'B-PRT': -9.947408739594056,\n",
       "  'I-VP': -11.903236325306716,\n",
       "  'B-ADJP': -10.568235258574376,\n",
       "  'B-SBAR': -9.769727562356604,\n",
       "  'B-ADVP': -11.451486136370331,\n",
       "  'I-INTJ': -9.981697813072689,\n",
       "  'B-CONJP': -6.591673732008659,\n",
       "  'I-CONJP': -6.591673732008659,\n",
       "  'I-ADVP': -9.444305161921976,\n",
       "  'I-ADJP': -9.62019582838564,\n",
       "  'I-SBAR': -7.10249935577465,\n",
       "  'I-PP': -7.438971592395863},\n",
       " 13: {'O': -14.284243380076564,\n",
       "  'B-INTJ': -6.591673732008659,\n",
       "  'B-PP': -6.591673732008659,\n",
       "  'B-NP': -6.591673732008659,\n",
       "  'I-NP': -6.591673732008659,\n",
       "  'B-VP': -6.591673732008659,\n",
       "  'B-PRT': -6.591673732008659,\n",
       "  'I-VP': -6.591673732008659,\n",
       "  'B-ADJP': -6.591673732008659,\n",
       "  'B-SBAR': -6.591673732008659,\n",
       "  'B-ADVP': -6.591673732008659,\n",
       "  'I-INTJ': -11.0803101017408,\n",
       "  'B-CONJP': -6.591673732008659,\n",
       "  'I-CONJP': -6.591673732008659,\n",
       "  'I-ADVP': -6.591673732008659,\n",
       "  'I-ADJP': -6.591673732008659,\n",
       "  'I-SBAR': -6.591673732008659,\n",
       "  'I-PP': -6.591673732008659},\n",
       " 14: {'O': -14.284243380076564,\n",
       "  'B-INTJ': -12.144633316930276,\n",
       "  'B-PP': -13.166749572608278,\n",
       "  'B-NP': -14.51791525517962,\n",
       "  'I-NP': -14.143911019569462,\n",
       "  'B-VP': -13.783103062045038,\n",
       "  'B-PRT': -11.046021028262167,\n",
       "  'I-VP': -13.001848613974825,\n",
       "  'B-ADJP': -11.666847547242487,\n",
       "  'B-SBAR': -10.868339851024714,\n",
       "  'B-ADVP': -12.550098425038442,\n",
       "  'I-INTJ': -11.0803101017408,\n",
       "  'B-CONJP': -7.690286020676769,\n",
       "  'I-CONJP': -7.690286020676769,\n",
       "  'I-ADVP': -10.542917450590085,\n",
       "  'I-ADJP': -10.71880811705375,\n",
       "  'I-SBAR': -8.20111164444276,\n",
       "  'I-PP': -8.537583881063973},\n",
       " 15: {'O': -11.375522483512205,\n",
       "  'B-INTJ': -7.690286020676769,\n",
       "  'B-PP': -7.690286020676769,\n",
       "  'B-NP': -7.690286020676769,\n",
       "  'I-NP': -7.690286020676769,\n",
       "  'B-VP': -7.690286020676769,\n",
       "  'B-PRT': -7.690286020676769,\n",
       "  'I-VP': -7.690286020676769,\n",
       "  'B-ADJP': -7.690286020676769,\n",
       "  'B-SBAR': -7.690286020676769,\n",
       "  'B-ADVP': -7.690286020676769,\n",
       "  'I-INTJ': -7.690286020676769,\n",
       "  'B-CONJP': -7.690286020676769,\n",
       "  'I-CONJP': -7.690286020676769,\n",
       "  'I-ADVP': -7.690286020676769,\n",
       "  'I-ADJP': -7.690286020676769,\n",
       "  'I-SBAR': -7.690286020676769,\n",
       "  'I-PP': -7.690286020676769},\n",
       " 16: {'STOP': -7.690286020676769}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes in list of tag sequences in the form [ [y1_1, y1_2, ... ], [y2_1, y2_2, ...], ... ]\n",
    "# outputs transition_dict, start_stop_transition_dict\n",
    "def create_transition_dict(input_list):\n",
    "    # Create transition dict\n",
    "    # ========================\n",
    "    transition_dict = {}\n",
    "    \n",
    "    # Get list of tags\n",
    "    tags = set([tag for sentence in input_list for tag in sentence])\n",
    "\n",
    "    # Update probability for each instance of tag1 > tag2 inside transition_dict\n",
    "    for tag1 in tags:\n",
    "        for tag2 in tags:\n",
    "            count = 0\n",
    "            total = 0\n",
    "            for sentence in input_list:\n",
    "                total += len(sentence) - 1\n",
    "                for i in range(len(sentence) - 1):\n",
    "                    if sentence[i] == tag1 and sentence[i+1] == tag2:\n",
    "                        count += 1\n",
    "            if count > 0:\n",
    "                transition_dict[(tag1, tag2)] = count / total\n",
    "    \n",
    "    # Create initial probability dict\n",
    "    # ========================\n",
    "    start_tag_count_dict = {}\n",
    "    stop_tag_count_dict = {}\n",
    "    \n",
    "    start_stop_transition_dict = {}\n",
    "    \n",
    "    # Get num of starting tags that appear\n",
    "    for sentence in input_list:\n",
    "        if sentence[0] not in start_tag_count_dict:\n",
    "            start_tag_count_dict[sentence[0]] = 1\n",
    "        else:\n",
    "            start_tag_count_dict[sentence[0]] += 1\n",
    "        \n",
    "        if sentence[-1] not in stop_tag_count_dict:\n",
    "            stop_tag_count_dict[sentence[-1]] = 1\n",
    "        else:\n",
    "            stop_tag_count_dict[sentence[-1]] += 1\n",
    "    \n",
    "    # Fill in initial prob dict with num of starting/ending tags divided by total sentence num\n",
    "    for tag in start_tag_count_dict:\n",
    "        start_stop_transition_dict[('START',tag)] = start_tag_count_dict[tag] / len(input_list)\n",
    "    for tag in stop_tag_count_dict:\n",
    "        start_stop_transition_dict[(tag,'STOP')] = stop_tag_count_dict[tag] / len(input_list)\n",
    "    \n",
    "    return transition_dict, start_stop_transition_dict\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# function that takes in observed variable x and hidden state y\n",
    "# returns transition parameter q(yi|yi-1)\n",
    "def transition(transition_dict, yi_minus_1, yi):\n",
    "    if (yi_minus_1, yi) not in transition_dict.keys():\n",
    "        transition_dict[(yi_minus_1, yi)] = 0\n",
    "\n",
    "    result = transition_dict[(yi_minus_1, yi)]\n",
    "\n",
    "    return result\n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# emission_dict, tags_list = create_emission_dict_tags_list(training_filename, k)\n",
    "# transition_dict, start_stop_transition_dict = create_transition_dict(training_filename)\n",
    "\n",
    "# creates the table of pi values\n",
    "def viterby_first_order(word_sequence, tags_list, emission_dict, transition_dict, start_stop_transition_dict):\n",
    "    pi_dict = {}\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # initialization\n",
    "    pi_dict[0] = {}\n",
    "    \n",
    "    for tag in tags_list:\n",
    "        pi_dict[0][tag] = 0\n",
    "        \n",
    "    pi_dict[0][\"START\"] = 1\n",
    "    \n",
    "    for word_index in range(1, len(word_sequence)+1): # index for each word in each sentence\n",
    "        for tag in tags_list: # each tag\n",
    "            if word_index not in pi_dict.keys():\n",
    "                pi_dict[word_index] = {}\n",
    "\n",
    "            pi_dict[word_index][tag] = 0 # initialize pi(j, u) = 0 for all j and u\n",
    "\n",
    "    # =============================================================================================================================\n",
    "    # intermediate steps\n",
    "    # for each observed variable\n",
    "    for j in range(0, len(word_sequence)):\n",
    "        x_j_plus_1 = word_sequence[j][0] # refers to the jth word (to calculate emission)\n",
    "        \n",
    "        # for each hidden state v\n",
    "        for v in tags_list:\n",
    "            \n",
    "            # pi(j+1, v) = max over all u { pi(j,u) * transition(u, v) * emissision(x_j_plus_1, v) }\n",
    "            max_pi_val = float('-inf')\n",
    "            \n",
    "            for u in tags_list:\n",
    "                pi = pi_dict[j][u]\n",
    "                trans = transition(transition_dict, u, v)\n",
    "                emi = emission(emission_dict, tags_list, x_j_plus_1, v)\n",
    "\n",
    "                if trans != 0:\n",
    "                    log_trans = math.log(trans)\n",
    "                else:\n",
    "                    log_trans = 0\n",
    "                \n",
    "                if emi != 0:\n",
    "                    log_emi = math.log(emi)\n",
    "                else:\n",
    "                    log_emi = 0\n",
    "                \n",
    "                current_pi_val = pi + log_trans + log_emi\n",
    "\n",
    "                # save the value that maximises\n",
    "                if (current_pi_val > max_pi_val):\n",
    "                    max_pi_val = current_pi_val\n",
    "        \n",
    "            pi_dict[j+1][v] = max_pi_val\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # final step\n",
    "    max_pi_val = float('-inf')\n",
    "    \n",
    "    # for each hidden state u\n",
    "    for u in tags_list:\n",
    "        pi = pi_dict[len(word_sequence)][u]\n",
    "        trans = transition(transition_dict, u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        if pi > 0:\n",
    "            pi = math.log(pi)\n",
    "\n",
    "        current_pi_val = pi + trans\n",
    "        \n",
    "        # save the value that maximises\n",
    "        if (current_pi_val > max_pi_val):\n",
    "            max_pi_val = current_pi_val\n",
    "        \n",
    "    pi_dict[len(word_sequence) + 1] = {}\n",
    "    pi_dict[len(word_sequence) + 1][\"STOP\"]  = max_pi_val\n",
    "    \n",
    "    # =============================================================================================================================\n",
    "    # backtracking\n",
    "    # check final layer argmax\n",
    "    argmax = float('-inf')\n",
    "    currentmax = 0\n",
    "    argmax_index = 0\n",
    "    \n",
    "    \n",
    "    for u in tags_list:\n",
    "        pi = pi_dict[len(word_sequence)][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        \n",
    "        if pi == 0:\n",
    "            pi = float('-inf')\n",
    "        \n",
    "        currentmax = pi + trans\n",
    "        \n",
    "        if currentmax > argmax:\n",
    "            argmax = currentmax\n",
    "            argmax_index = u\n",
    "        \n",
    "    decoding_list.append(argmax_index)\n",
    "    \n",
    "    \n",
    "    # Backtrack rest of pi_dict\n",
    "    for j in range(len(pi_dict)-2, 0, -1):\n",
    "        \n",
    "        argmax = float('-inf')\n",
    "        currentmax = 1\n",
    "        argmax_index = 0\n",
    "    \n",
    "        for u in tags_list_w_start_stop:\n",
    "    \n",
    "            pi = pi_dict[j][u]\n",
    "            trans = transition(u, decoding_list[-1])\n",
    "            \n",
    "            if trans != 0:\n",
    "                trans = math.log(trans)\n",
    "            if pi == 0:\n",
    "                pi = float('-inf')\n",
    "\n",
    "            currentmax = pi + trans\n",
    "\n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_index = u\n",
    "        \n",
    "        decoding_list.append(argmax_index)\n",
    "        \n",
    "    decoding_list = decoding_list[::-1]\n",
    "    \n",
    "    write_result_viterby(test_filename, result_filename, decoding_list)\n",
    "            \n",
    "    return decoding_list\n",
    "\n",
    "\n",
    "# print(transition_dict)\n",
    "# print(emission_dict)\n",
    "\n",
    "training_filename = \"EN/train\"\n",
    "emission_dict, tags_list = create_emission_dict_tags_list(training_filename, 1)\n",
    "transition_dict, start_stop_transition_dict = create_transition_dict(training_filename)\n",
    "\n",
    "word_sequences = read_test_data(\"EN/dev.in\")\n",
    "\n",
    "word_sequence = word_sequences[0]\n",
    "\n",
    "viterby_first_order(word_sequence, tags_list, emission_dict, transition_dict, start_stop_transition_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_viterby(test_filename, result_filename, results):\n",
    "    \n",
    "    test_data = read_test_data(test_filename)\n",
    "    \n",
    "    with open(result_filename, \"w\" ,encoding=\"utf-8\") as fp:\n",
    "        \n",
    "        for word,tag in zip(test_data, results):\n",
    "            # account for empty lines\n",
    "            if(len(word) == 0):\n",
    "                fp.write(\"\\n\")\n",
    "            else:\n",
    "                fp.write(word[0] + \" \" + tag + \"\\n\")\n",
    "    fp.close()\n",
    "    \n",
    "# -----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def write_result(result_filename, results):\n",
    "    result_file = open(result_filename, \"w\" ,encoding=\"utf-8\")\n",
    "    \n",
    "    for result in results:\n",
    "        # account for empty lines\n",
    "        if (len(result) == 0):\n",
    "            result_file.write(\"\\n\")\n",
    "        else:\n",
    "            result_file.write(result[0] + \" \" + result[1] + \"\\n\")\n",
    "\n",
    "    result_file.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterby_backtracking(test_filename, result_filename):\n",
    "    global transition_dict\n",
    "    global pi_dict\n",
    "    global decoding_list\n",
    "    \n",
    "    \n",
    "\n",
    "    tags_list_w_start_stop = list(tags_list)\n",
    "    \n",
    "    # check final layer argmax\n",
    "    argmax = float('-inf')\n",
    "    currentmax = 0\n",
    "    argmax_index = 0\n",
    "    \n",
    "    for u in tags_list_w_start_stop:\n",
    "        pi = pi_dict[len(pi_dict)-1][u]\n",
    "        trans = transition(u, \"STOP\")\n",
    "        \n",
    "        if trans != 0:\n",
    "            trans = math.log(trans)\n",
    "        \n",
    "        if pi == 0:\n",
    "            pi = float('-inf')\n",
    "        \n",
    "        currentmax = pi + trans\n",
    "        \n",
    "        if currentmax > argmax:\n",
    "            argmax = currentmax\n",
    "            argmax_index = u\n",
    "        \n",
    "    decoding_list.append(argmax_index)\n",
    "    \n",
    "    \n",
    "    # Backtrack rest of pi_dict\n",
    "    for j in range(len(pi_dict)-2, 0, -1):\n",
    "        \n",
    "        argmax = float('-inf')\n",
    "        currentmax = 1\n",
    "        argmax_index = 0\n",
    "    \n",
    "        for u in tags_list_w_start_stop:\n",
    "    \n",
    "            pi = pi_dict[j][u]\n",
    "            trans = transition(u, decoding_list[-1])\n",
    "            \n",
    "            if trans != 0:\n",
    "                trans = math.log(trans)\n",
    "            if pi == 0:\n",
    "                pi = float('-inf')\n",
    "\n",
    "            currentmax = pi + trans\n",
    "\n",
    "            if currentmax > argmax:\n",
    "                argmax = currentmax\n",
    "                argmax_index = u\n",
    "        \n",
    "        decoding_list.append(argmax_index)\n",
    "        \n",
    "    decoding_list = decoding_list[::-1]\n",
    "    \n",
    "    write_result_viterby(test_filename, result_filename, decoding_list)\n",
    "            \n",
    "    return decoding_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write_result() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-06e02f7d7108>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# perform prediction for the EN dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0men_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_sentiment_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"EN/train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"EN/dev.in\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"EN/dev.p1.out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# evaluate prediction for the EN dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-eee7026a4288>\u001b[0m in \u001b[0;36msimple_sentiment_analysis\u001b[1;34m(training_filename, test_filename, result_filename, k)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mcurrent_prediction_tag_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mwrite_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_word_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_tag_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_word_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_tag_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memission_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: write_result() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# perform prediction for the EN dataset\n",
    "en_results = simple_sentiment_analysis(\"EN/train\", \"EN/dev.in\", \"EN/dev.p1.out\")\n",
    "\n",
    "# evaluate prediction for the EN dataset\n",
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterby_first_order(\"EN/train\", \"EN/dev.in\")\n",
    "viterby_backtracking(\"EN/dev.in\", \"EN/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"EN/dev.out\" \"EN/dev.p2.out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "emission_dict = {} # emission_dict[x][y] gives e(x|y)\n",
    "tags_list = []\n",
    "transition_dict = {}\n",
    "pi_dict = {}\n",
    "decoding_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform prediction for the FR dataset\n",
    "fr_results = simple_sentiment_analysis(\"FR/train\", \"FR/dev.in\", \"FR/dev.p1.out\")\n",
    "\n",
    "# # evaluate prediction for the FR dataset\n",
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p1.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viterby_first_order(\"FR/train\", \"FR/dev.in\")\n",
    "viterby_backtracking(\"FR/dev.in\", \"FR/dev.p2.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python \"evalResult.py\" \"FR/dev.out\" \"FR/dev.p2.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
